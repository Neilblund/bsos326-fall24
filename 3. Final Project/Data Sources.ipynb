{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54cdf517-0d82-4fe5-910f-41cc914e072e",
   "metadata": {},
   "source": [
    "# Some free APIs that you could consider for this course\n",
    "\n",
    "This is by no means an exhaustive list! I'm a political scientist, and my selections here reflect that bias. If you're not interested in that, then you should go looking for an alternative. There are lots of freely available APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c49f38-8302-4d88-b872-e5db5c7f1155",
   "metadata": {},
   "source": [
    "These sources have Application Programming Interfaces that you can access using R code. Some of them may have an R \"wrapper\" that simplifies the process of gathering data from them and putting it into a data frame. I've listed some of the more useful ones here, but keep in mind that many legislatures, government agencies, or international organizations offer an API now, so don't treat this as a definitive list.\n",
    "\n",
    "[Check this list of free APIs](https://github.com/public-apis/public-apis) \n",
    "\n",
    "-   [Data.gov](https://api.data.gov/): Data from U.S. Federal Agencies\n",
    "\n",
    "-   [BLS](https://www.bls.gov/developers/): U.S. Bureau of Labor Statistics\n",
    "\n",
    "-   [UCDP](https://ucdp.uu.se/) : Global data on violent conflict and protest. Updates yearly.\n",
    "\n",
    "-   [ACLED](https://apidocs.acleddata.com/) : Violent conflict and protest. Updates weekly.\n",
    "\n",
    "-   [Congress.gov](https://gpo.congress.gov/) : U.S. Congress\n",
    "\n",
    "-   [UK Parliament](https://developer.parliament.uk/): (really, lots of legislative bodies have an API now, I won't list them all here)\n",
    "\n",
    "-   [World Bank](https://datatopics.worldbank.org/world-development-indicators/) (especially development indicators) are a good source for background data on things like GDP, literacy, etc.\n",
    "\n",
    "-   [Manifestos Project](https://manifesto-project.wzb.eu/information/documents/api): Party Manifestos from across the world. Many of these have been split by sentence and then each statement has been manually categorized by topic. \n",
    "\n",
    "-   [OECD](https://data.oecd.org/api/) : Mostly economic data on OECD member states.\n",
    "\n",
    "-   [Spotify](https://developer.spotify.com/documentation/web-api) This requires an access token. You can see a guide for how this might be used at https://medium.com/@maxtingle/getting-started-with-spotifys-api-spotipy-197c3dc6353b\n",
    "\n",
    "-   [Reddit](https://www.reddit.com/dev/api) (see https://www.jcchouinard.com/reddit-api-without-api-credentials/ for an example of how this might work)\n",
    "\n",
    "-   [Housing Market API](https://documenter.getpostman.com/view/9197254/UVsFz93V#quickstart)\n",
    "\n",
    "-   [Delphi CovidCast](https://cmu-delphi.github.io/delphi-epidata/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df6c65b-4fc8-4c08-99f6-a60fcda827cd",
   "metadata": {},
   "source": [
    "# Tips on finding a website to scrape\n",
    "\n",
    "If you're interested in going the webscraping route, I would suggest looking for a site that is going to be reasonably easy to scrape. Simpler is better! Some sites may even have a \"simplified version for slow connections\", these are going to be much easier compared to sites that have really complicated HTML. Also keep in mind that sites that require you to enter a password or login are generally not going to be amenable to web scraping. \n",
    "\n",
    "If you're planning to scrape text data, then you want to look for a site where you can get a lot of links. The websites for major news outlets are always a good option. Blogs and press release pages can also be good: for instance, every member of the U.S. Senate has a website, and most of them will post press releases on a somewhat regular basis and these will usually have a consistent html structure that you can parse.\n",
    "\n",
    "## Using a sitemap\n",
    "\n",
    "Larger sites will usually have a sitemap that acts as a map of URLs to make it easier to web crawlers to index pages. For instance, the Associated Press has a map for\n",
    "<a href=https://apnews.com/ap-sitemap-202410.xml>stories from October 2024 here</a> (this will probably load slowly!)\n",
    "\n",
    "I can use the sitemap to create a list of links, write a scraper that can extract the text from each page, and then create a loop that will extract the relevant text data from each article. I've included an example of doing this for a handful of articles below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6c8c46-156d-42f5-81b0-1f1aeaeb35a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages \n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "import lxml\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1342273-7f71-4394-a932-522a9aaa7bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sitemap\n",
    "october_sitemap = get('https://apnews.com/ap-sitemap-202410.xml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d228c0-c615-44b4-a008-97448ccf7faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the content as an XML document\n",
    "sitemap= BeautifulSoup(october_sitemap.content, features=\"xml\")\n",
    "\n",
    "# select all <loc> nodes that are descendants of a url node\n",
    "url_nodes = sitemap.select('url loc')\n",
    "\n",
    "# loop through the entire list and just get the link\n",
    "urls = [i.get_text() for i in url_nodes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1c3418-d99d-4d25-a806-4de7d6ba370d",
   "metadata": {},
   "source": [
    "The links here contain a bunch of different article types, but maybe I only want the articles and not any of the links to video links or 'hubs'. I can use a regular expression to detect the urls that have \"article\" as part of their path and create a list with only these links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446cca2c-8a4d-4bab-a3e2-84c7c52e35f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_urls = []\n",
    "[article_urls.append(i) if bool(re.search(\"/article/\", i)) else ''  for i in urls]\n",
    "article_urls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ab25cd-f8f4-4cd7-aec4-8e60ef4fa3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.DataFrame(article_urls, columns = ['url'])\n",
    "\n",
    "articles[\"headline\"] = np.nan\n",
    "articles[\"article_text\"] = np.nan\n",
    "\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c74a2ce-99c4-45cc-a750-425f12685506",
   "metadata": {},
   "source": [
    "Now I would just need to write a loop to visit each of these urls, extract the information I'm interested in, and put the result in a dataframe. As a courtesy, you probably should also try to limit the frequency of your requests. A simple way to do this is to put a `time.sleep()` function inside your loop, which will cause it to pause for a number of seconds after each iteration.\n",
    "\n",
    "In the interest of speeding things along, I'm just going to grab the first 5 urls here, but ideally we would want to capture everything and then store it somewhere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97c307a-d998-45f9-b2c6-ce8768947457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range(5):\n",
    "    # visit url i\n",
    "    req = get(articles['url'][i])\n",
    "    # extract the html\n",
    "    article= BeautifulSoup(req.content)\n",
    "    # get the headline and place it in row i\n",
    "    articles.loc[i, \"headline\"] = ' '.join([str(i.get_text()) for i in article.select(\"h1.Page-headline\")])\n",
    "    # get the text and place it in row i \n",
    "    articles.loc[i, \"article_text\"] = ' '.join([str(i.get_text()) for i in article.select(\".Page-storyBody p\")])\n",
    "    # pause for one second after each iteration of the loop:\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3cd96c-cbf0-4893-8781-ad156f643104",
   "metadata": {},
   "source": [
    "Once this runs, I should have article text and headlines in my data frame. Which I can now use for further analysis. (Note: I would probably also want to write some code to get the publication date and maybe the author name here as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f0eb20-fb9c-4fb6-a33a-d3ed214a6fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
