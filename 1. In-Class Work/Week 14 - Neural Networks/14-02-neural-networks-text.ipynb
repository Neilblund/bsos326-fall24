{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9fbb3d1-da93-4b6c-a574-73e09ec2b561",
   "metadata": {},
   "source": [
    "# Neural Networks II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65352a-adde-48f5-bc2a-d5691291a3fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install keras_nlp\n",
    "%pip install tensorflow_datasets\n",
    "%pip install transformers\n",
    "%pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "id": "9115f60f-17a7-4f73-9deb-aabce62a388d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, classification_report, balanced_accuracy_score\n",
    "\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6a31b6-4671-4bc1-9b90-db7252128f13",
   "metadata": {},
   "source": [
    "## Neural Networks for Text Data\n",
    "\n",
    "Neural networks are extremely flexible, which allows you to use them for all kinds of data. We've already seen this with data that was in a 2-dimensional format with images. They can also be used for text data to do tasks such as sentiment analysis using supervised learning.\n",
    "\n",
    "\n",
    "\n",
    "Go ahead and run this (it will take a moment to finish) and we'll talk about it in a moment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "id": "b84a44ba-a171-497d-a5b9-ce6ebaadc356",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.load(\"https://www.kaggle.com/models/google/universal-sentence-encoder/TensorFlow2/universal-sentence-encoder/2\")\n",
    "# this does nothing except fix a compatibility issue with tensorflow\n",
    "embed_layer_wrapper = tf.keras.layers.Lambda(lambda x: embed(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a458dd74-4ff6-404f-bc0a-b1eebd7a05de",
   "metadata": {},
   "source": [
    "# Reviews\n",
    "\n",
    "We'll start by reloading the IMDB movie review corpus that we used a couple of weeks ago. Just to refresh your memory: this is a subset from a larger corpus of user generated IMDB reviews. The dataset contains the full text of each review, along with a numeric label that is equal to 0 if the review was negative and 1 if the review is positive. Because this is just an example data set, there's actually an even split between positive and negative reviews here, so we have a more-or-less balanced sample of 2500 positive reviews and 2500 negative reviews to work with:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "id": "c45fa26c-1324-45fb-a23b-a2a88131f0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I always wrote this series off as being a comp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1st watched 12/7/2002 - 3 out of 10(Dir-Steve ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This movie was so poorly written and directed ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The most interesting thing about Miryang (Secr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when i first read about \"berlin am meer\" i did...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I always wrote this series off as being a comp...      0\n",
       "1  1st watched 12/7/2002 - 3 out of 10(Dir-Steve ...      0\n",
       "2  This movie was so poorly written and directed ...      0\n",
       "3  The most interesting thing about Miryang (Secr...      1\n",
       "4  when i first read about \"berlin am meer\" i did...      0"
      ]
     },
     "execution_count": 789,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_csv(\"imdb_reviews.csv\")\n",
    "\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6dddb5-9a07-40bc-91ec-6236df315551",
   "metadata": {},
   "source": [
    "Just as in previous classes, we're going to be evalauting a model here by creating separate training and testing datasets. We'll also convert these datasets to tensors in order to make it easier to work with them in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "id": "dbd1f887-546e-4bb2-b7b8-243cd1d53dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_examples,  test_examples= train_test_split(reviews,\n",
    "                                     test_size=0.20, # 20% of observations for validation\n",
    "                                     random_state = 999) # this is a random process, so you want to set a random seed! \n",
    "\n",
    "\n",
    "# convert to tensor objects\n",
    "train_tensor = tf.convert_to_tensor(train_examples['text'])\n",
    "test_tensor = tf.convert_to_tensor(test_examples['text'])\n",
    "train_labels = tf.convert_to_tensor(train_examples['label'])\n",
    "test_labels = tf.convert_to_tensor(test_examples['label'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "id": "39f76073-bab0-4697-9642-c8d5363e3bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training entries: 4000, test entries: 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training entries: {len(train_examples)}, test entries: {len(test_examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cac3d0-f5ec-49ff-8cbe-8d8c1995e064",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "In a previous class, we trained a naive bayes classifier to distinguish positive from negative IMDB reviews with a fairly high degree (~84%) accuracy. \n",
    "\n",
    "Now, we're going to try to do the same task using a neural network trained on a sentence embedding model. **Text Embeddings** represent one way that analysts can move away from the bag-of-words model to create classifiers that can account for things like word order, synonyms and antonyms and complex grammatical relationships.\n",
    "\n",
    "Word/Sentence/Document embedding models can take strings of text and convert them into a \"dense\" vector of numbers whose values reflect some kind of abstract meaning. The precise method for creating them will be different depending on the model, but the general idea is that they use some text as training data and then are trained to \"predict\" some missing text or context. The weights from this predictive model will be similar for texts that have similar meanings. \n",
    "\n",
    "In a well-trained word-embedding model, words with similar meanings will have similar values (<a href='https://projector.tensorflow.org/'>there's a good visual representation here</a>). Instead of using a bag-of-words as our input for a classifier, we can pass our text through an embedding model to get a representation that can account for things like synonyms and context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e64c139-d717-4abb-9f11-87c405a42d8c",
   "metadata": {},
   "source": [
    "The `embed` object we downloaded earlier is a pre-trained embedding model that is built for general-purpose applications. It takes a list of strings as inputs and returns a vector of 512 numbers that represent that sentences \"location\" in a 512 dimension space. Here's an example of getting the first ten elements from the embedding for a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "id": "579b3988-e3d0-41b7-8092-049b5f55affb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       "array([-3.59944254e-02,  2.86443513e-02,  5.79423613e-05, -1.09751895e-02,\n",
       "       -3.56823625e-03,  2.59994646e-03,  1.08064972e-02, -1.86106842e-02,\n",
       "       -2.18271017e-02, -2.75516417e-02], dtype=float32)>"
      ]
     },
     "execution_count": 795,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding a sentence about catci and looking at the first 10 elements\n",
    "\n",
    "embed([\"The rattail cactus is native to Mexico.\"])[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dae75a-04fc-448b-a3b7-6d54a87d301c",
   "metadata": {},
   "source": [
    "To illustrate what why this is useful, we can use a little code from the <a href='https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder'>online documentation</a> that will allow us to visualize the similarities between the embeddings produced by different sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "id": "3453094c-f41f-4ab3-8cca-0ab8f6a3cdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity(labels, features, rotation):\n",
    "  corr = np.inner(features, features)\n",
    "  sns.set(font_scale=1)\n",
    "  g = sns.heatmap(\n",
    "      corr,\n",
    "      xticklabels=labels,\n",
    "      yticklabels=labels,\n",
    "      vmin=0,\n",
    "      vmax=1,\n",
    "      cmap=\"YlOrRd\")\n",
    "  g.set_xticklabels(labels, rotation=rotation)\n",
    "  g.set_title(\"Semantic Textual Similarity\")\n",
    "\n",
    "def run_and_plot(messages_):\n",
    "  message_embeddings_ = embed(messages_)\n",
    "  plot_similarity(messages_, message_embeddings_, 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4339248-63d3-471d-9efa-00f74ac80be8",
   "metadata": {},
   "source": [
    "Below are some sentences from different wikipedia entries. The first two are from the entry on *Citizen Kane*, the last two are from entries on cacti. Note that the terms in both groups share very few terms overall, but take a look at their similarities as measured by the innner products of their respective embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a42284b-8f99-47d2-9a06-ce25eddccf16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_and_plot([\n",
    "    # two sentences from the Wikipedia entry for citizen kane\n",
    "    \"Citizen Kane is often cited as the greatest film ever.\",\n",
    "    \"Hollywood had shown interest in Welles as early as 1936.\",\n",
    "    # sentences from entries on cacti\n",
    "    \"The rattail cactus is native to Mexico.\",\n",
    "    \"Prickly pears are frequently found around California.\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786b453b-8aad-40b3-9810-0bf114481fc7",
   "metadata": {},
   "source": [
    "In essence, text embeddings give us a more flexibile way to represent text that can account for nuanced aspects of meaning and context, so that sentences about the same general idea are \"close\" in the embedding space even if they share none of the exact same terms. Feeding these inputs - instead of a simple bag of words - into a machine learning model, can allow us to make more effective use of the same data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e3e000-fe1b-43a3-a9b0-fc5b4cccd163",
   "metadata": {},
   "source": [
    "## Fitting the model\n",
    "\n",
    "Now, let's fit a model to predict movie reviews that uses the embedding model. We'll use the embedding layer as our input layer and then include two hidden layers and a sigmoid output layer that will return our predicted probability of a review being negative or positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "id": "a6ccc22b-1b82-4650-a744-26e3842d2604",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_49\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_49\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lambda_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                   │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_135 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_136 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_137 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lambda_22 (\u001b[38;5;33mLambda\u001b[0m)                   │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_135 (\u001b[38;5;33mDense\u001b[0m)                    │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_136 (\u001b[38;5;33mDense\u001b[0m)                    │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_137 (\u001b[38;5;33mDense\u001b[0m)                    │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "   \n",
    "    embed_layer_wrapper,\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbf1c5c-3195-46d2-a084-18c6162ebe29",
   "metadata": {},
   "source": [
    "After that, we compile our model and then train it for 15 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843dbed4-ad1a-48a6-816a-d78b3ecaedbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(train_tensor, \n",
    "                    train_labels,\n",
    "                    epochs=15,\n",
    "                    batch_size=500,\n",
    "                    validation_data=(test_tensor, \n",
    "                                     test_labels),\n",
    "                    verbose=1\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a3d89e-993d-45d4-a5cc-81ec4116fce9",
   "metadata": {},
   "source": [
    "Now we can generate some predictions and look at our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22c7891-ae48-49d5-abcd-21c19ff63612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions from test data\n",
    "preds = model.predict(test_tensor).flatten()>=.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed930a3-764b-4538-9921-0e5ac509f2de",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\">Q1:Create a confusion matrix and classification report from the predictions and assess the quality of the classifier</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8669f70-3bdd-4db6-82a9-63b104b4b8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a confusion matrix\n",
    "cmat =pd.crosstab(test_labels, preds>=.5,  margins=True).rename_axis(index = 'Truth', columns='Predictions')\n",
    "cmat\n",
    "\n",
    "# evaluate the performance on the held out data\n",
    "print(classification_report(test_labels, preds>=.5, \n",
    "                            # add target_names to show labels in the report:\n",
    "                              target_names=['Negative', 'Positive']))\n",
    "\n",
    "\n",
    "# add cohen's kappa and balanced accuracy\n",
    "print(\"cohens kappa: \", cohen_kappa_score(test_labels, preds>=.5))\n",
    "print(\"balanced accuracy: \", balanced_accuracy_score(test_labels, preds>=.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9844f15b-406c-46a4-8cd0-a446dea60eb1",
   "metadata": {},
   "source": [
    "How does this do? Does it outperform the naive bayes classifier? Why might this be? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f80381-6aaa-4942-b0e9-423ad700cdc0",
   "metadata": {},
   "source": [
    "## Changes to the Model\n",
    "\n",
    "We can make changes to the model to add more layers, use more nodes, train it for longer, or even use a different kind of model. This is part of the overall process for finding the model that has the best performance in terms of accuracy. In reality, we would do these steps many, many times, tuning our model so that it is as good as possible.\n",
    "\n",
    "In reality, the full IMDB reviews corpus is much larger than what we've been using here, so we would also want to use that data in its entirety for a real world application, but since that takes a while to train, we can use a pre-made model that was trained on this data set to get a sense of how well we could do if we did some more fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64d96b7-3c44-40c8-8055-578f21947969",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\">Q2: Change something about the model above and compare your results. </h3>\n",
    "\n",
    "(Some options are: add an additional hidden layer, run the same model for more epochs, add more nodes to one or more of the layers, or add <a href='https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout'>dropout</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abc62f2-8352-42bd-935e-6581a3252230",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4409f940-d869-48a1-ae1b-4def25ab9dc0",
   "metadata": {},
   "source": [
    "### Pre-built models from Hugging Face\n",
    "\n",
    "The [Hugging Face Hub](https://huggingface.co/models) has many models that have been pre-trained for you to use. One of the models hosted there is a <a href='https://huggingface.co/aychang/roberta-base-imdb'>sentiment classifer that was trained on the entire IMDB corpus</a>. We can load this model and see how it performs on our held-out data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0bdf53-ad36-41f8-9dbf-d0245d434650",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tiny_bert = pipeline(\"text-classification\", \"arnabdhar/tinybert-imdb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "750bde1a-df2d-469d-b063-ba36b689708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to list since thats the input format this model uses\n",
    "test_list = test_examples['text'].tolist()\n",
    "\n",
    "# applying the model\n",
    "\n",
    "results = tiny_bert(inputs =test_list, max_length=512, truncation=True)\n",
    "# looking at the first five results\n",
    "results[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "c880fb6f-bfdf-4113-92cf-6be63fe4e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformatting to match our original test labels \n",
    "tiny_bert_preds = [int(i['label']==\"POSITIVE\") for i in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "de4422e8-fb46-493f-aba8-b28a0a8d2b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.94      0.95      0.94       499\n",
      "    Positive       0.95      0.94      0.94       501\n",
      "\n",
      "    accuracy                           0.94      1000\n",
      "   macro avg       0.94      0.94      0.94      1000\n",
      "weighted avg       0.94      0.94      0.94      1000\n",
      "\n",
      "cohens kappa:  0.8860018239708165\n",
      "balanced accuracy:  0.9430097720390882\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, tiny_bert_preds, \n",
    "                            # add target_names to show labels in the report:\n",
    "                              target_names=['Negative', 'Positive']))\n",
    "\n",
    "\n",
    "# add cohen's kappa and balanced accuracy\n",
    "print(\"cohens kappa: \", cohen_kappa_score(test_labels, tiny_bert_preds))#\n",
    "print(\"balanced accuracy: \", balanced_accuracy_score(test_labels, tiny_bert_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684505cd-6bc2-4a6a-8cca-bca2fd36c5b9",
   "metadata": {},
   "source": [
    "## Other Types of Sentiment\n",
    "\n",
    "The nice thing about these models is that they are also pre-trained to do different types of sentiment analysis. For example, let's take the Distilbert-base-uncased-emotion model. This provides scores for emotions such as joy or anger. Here's an example of getting the emotions expressed in the first 100 rows the the reviews data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "id": "590b7962-541b-45a0-b596-3603dc1f760f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"text-classification\",\n",
    "                      model='bhadresh-savani/distilbert-base-uncased-emotion', \n",
    "                      top_k=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "id": "e03d9f39-8a99-49d7-961b-a93c0effe63c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction = classifier(test_list[:100], truncation=True, max_length=120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "id": "cc41d36e-c8a1-41d7-8322-5866ff0c1ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>median</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.995250</td>\n",
       "      <td>0.013120</td>\n",
       "      <td>0.187991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.986734</td>\n",
       "      <td>0.002353</td>\n",
       "      <td>0.103024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.998840</td>\n",
       "      <td>0.493655</td>\n",
       "      <td>0.513088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.644965</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>0.012831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.998333</td>\n",
       "      <td>0.005088</td>\n",
       "      <td>0.141641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.980268</td>\n",
       "      <td>0.001508</td>\n",
       "      <td>0.041424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             score                              \n",
       "               min       max    median      mean\n",
       "label                                           \n",
       "anger     0.000115  0.995250  0.013120  0.187991\n",
       "fear      0.000088  0.986734  0.002353  0.103024\n",
       "joy       0.000325  0.998840  0.493655  0.513088\n",
       "love      0.000161  0.644965  0.001724  0.012831\n",
       "sadness   0.000190  0.998333  0.005088  0.141641\n",
       "surprise  0.000139  0.980268  0.001508  0.041424"
      ]
     },
     "execution_count": 765,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_prediction = pd.concat([pd.DataFrame(i) for i in prediction])\n",
    "emotion_prediction.groupby('label').agg({'score':['min','max','median','mean']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "id": "e20c5eec-a6df-4de4-8b00-4cce204fba0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>label</th>\n",
       "      <th>anger</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.998835</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>i first saw this movie at the sundance film fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006406</td>\n",
       "      <td>0.003685</td>\n",
       "      <td>0.977704</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>0.002732</td>\n",
       "      <td>0.008680</td>\n",
       "      <td>The case history of 'Mulholland Dr.' is known:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.182236</td>\n",
       "      <td>0.722556</td>\n",
       "      <td>0.071156</td>\n",
       "      <td>0.002334</td>\n",
       "      <td>0.018521</td>\n",
       "      <td>0.003197</td>\n",
       "      <td>Despite having a very pretty leading lady (Ros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.998620</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>... than this ;-) What would happen if Terry G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.962185</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.003103</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.032253</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>Critics are falling over themselves within the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "label         anger      fear       joy      love   sadness  surprise  \\\n",
       "doc_index                                                               \n",
       "0          0.000151  0.000128  0.998835  0.000374  0.000190  0.000321   \n",
       "1          0.006406  0.003685  0.977704  0.000793  0.002732  0.008680   \n",
       "2          0.182236  0.722556  0.071156  0.002334  0.018521  0.003197   \n",
       "3          0.000267  0.000179  0.998620  0.000282  0.000375  0.000277   \n",
       "4          0.962185  0.001332  0.003103  0.000670  0.032253  0.000457   \n",
       "\n",
       "label                                                   text  \n",
       "doc_index                                                     \n",
       "0          i first saw this movie at the sundance film fe...  \n",
       "1          The case history of 'Mulholland Dr.' is known:...  \n",
       "2          Despite having a very pretty leading lady (Ros...  \n",
       "3          ... than this ;-) What would happen if Terry G...  \n",
       "4          Critics are falling over themselves within the...  "
      ]
     },
     "execution_count": 766,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l= []\n",
    "[l.extend([i] * 6) for i in range(len(prediction))]\n",
    "emotion_prediction['doc_index'] = l\n",
    "wide_fmt =emotion_prediction.reset_index().pivot(index = 'doc_index',columns='label', values='score')\n",
    "wide_fmt['text'] = test_list[:100]\n",
    "wide_fmt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d44fd4-7bc8-4a83-9486-747d961b8f78",
   "metadata": {},
   "source": [
    "You can check out some other options on the hugging face <a href='https://huggingface.co/models'>models page.</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
