{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9fbb3d1-da93-4b6c-a574-73e09ec2b561",
   "metadata": {},
   "source": [
    "# Neural Networks II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65352a-adde-48f5-bc2a-d5691291a3fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install keras_nlp\n",
    "# !pip install tensorflow_datasets\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9115f60f-17a7-4f73-9deb-aabce62a388d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6a31b6-4671-4bc1-9b90-db7252128f13",
   "metadata": {},
   "source": [
    "## Neural Networks for Text Data\n",
    "\n",
    "Neural networks are extremely flexible, which allows you to use them for all kinds of data. We've already seen this with data that was in a 2-dimensional format with images. They can also be used for text data to do tasks such as sentiment analysis using supervised learning.\n",
    "\n",
    "Let's take a look at an example. We'll bring in a dataset of IMDB reviews from the `tensorflow_datasets` package. This is a set of reviews of movies along with their labels of whether it was a positive review or a negative review. This data can be used to train a neural network model, which can then in turn be used on new movie reviews to determine their sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7ed08a-b8aa-4789-acb7-5fbf4cf7b50f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], \n",
    "                                  batch_size=-1, as_supervised=True)\n",
    "\n",
    "train_examples, train_labels = tfds.as_numpy(train_data)\n",
    "test_examples, test_labels = tfds.as_numpy(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e6d6b0-3377-43ab-8003-db53cdafd46e",
   "metadata": {},
   "source": [
    "Note that we unpack the text data from the actual labels. So, for our purposes, the `train_examples` is the \"X\" data with which we will train the model, and the `train_labels` is the \"y\" data which indicates whether it was positive or negative.\n",
    "\n",
    "Let's take a look at how many observations there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f76073-bab0-4697-9642-c8d5363e3bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Training entries: {len(train_examples)}, test entries: {len(test_examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a8e60a-6abc-4c4d-a945-7b1997c6b5b7",
   "metadata": {},
   "source": [
    "If we take a look at the data, we can see that it was the raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90773978-51bb-4e68-99ee-ad0d8f695604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_examples[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8e5edc-6212-457d-afc8-9642c06fed45",
   "metadata": {},
   "source": [
    "The labels are 0 or 1, with 0 representing negative and 1 representing positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29ed81b-b3d9-4804-a51a-a426e9fdfd9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cac3d0-f5ec-49ff-8cbe-8d8c1995e064",
   "metadata": {},
   "source": [
    "## Pre-Built Models\n",
    "\n",
    "Neural networks can take a long time to train and build. This is especially true for complicated models with complicated data, such as text data or image data. Luckily for us, people have taken the time to train models and build layers of models to use. We'll take one that has already been pre-built to take raw text data and converts it into text embedding vectors. You can think of this layer as doing something similar to the text processing that we've done before, such as tokenization. This has the added benefit of including the context of the words within the text as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2c9b4-450f-4b95-b77a-c79477f0d2e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "hub_layer = hub.KerasLayer(model, input_shape=[], dtype=tf.string, trainable=True)\n",
    "hub_layer(train_examples[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14ccbaf-7776-478a-b873-56cf4df98d9e",
   "metadata": {},
   "source": [
    "Now, let's fit the full model. We'll use the layer that we downloaded as the first layer, then add a simple Dense layer for now. We can do this similar to how we created the model structure. Note that here, we create the empty model object, then add layers. This is the same as creating the model all at once using a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ccc22b-1b82-4650-a744-26e3842d2604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbf1c5c-3195-46d2-a084-18c6162ebe29",
   "metadata": {},
   "source": [
    "After that, we compile our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0d53b3-d395-4514-a601-67cbb824f1a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[tf.metrics.BinaryAccuracy(threshold=0.5, name='accuracy')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898a98a2-91ea-4a9f-96c2-3dee092e3349",
   "metadata": {},
   "source": [
    "## Train-Validation Split\n",
    "\n",
    "One possible pitfall when training machine learning models is something called **overfitting**. Overfitting happens when you train a model that is too specific to the data that you are training with, and it ends up not generalizing to new data. \n",
    "\n",
    "In order to avoid issues with overfitting, we can use a **validation set** to see when our models stop improving and start overfitting. To do this, we simply split our data again, designating one as the main train data and the rest as the validation data. Then, we use the validation data to calculate our accuracy as we go, so that we can see when model stops improving on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d062860b-6c8b-413c-b056-e5b8ab37197b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_val = train_examples[:10000]\n",
    "partial_x_train = train_examples[10000:]\n",
    "\n",
    "y_val = train_labels[:10000]\n",
    "partial_y_train = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff79e62b-0551-49f5-8df7-3905a37aa2e9",
   "metadata": {},
   "source": [
    "Finally, we fit our data using the `model.fit` structure as before. We give it the validation data so that we can see the performance on the validation set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d135276-7535-4ef9-98d7-95ad19d0ef2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e40422f-855e-40b9-8793-29b2d9b9f6a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f87938-f6dc-4b9e-a5e9-41ac5bca92f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9924022b-f8b5-45f4-884a-0939ccf3aef3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f80381-6aaa-4942-b0e9-423ad700cdc0",
   "metadata": {},
   "source": [
    "## Changes to the Model\n",
    "\n",
    "We can make changes to the model to add more layers and use a different number of epochs. This is part of the overall process for finding the model that has the best performance in terms of accuracy. In reality, we would do these steps many, many times, tuning our model so that it is as good as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a6444f-b70c-4b84-8d2b-cd3046566297",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[tf.metrics.BinaryAccuracy(threshold=0.5, name='accuracy')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ae0d3e-ae18-4a2a-8cd8-2c2eee6dba5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39117dfd-8be9-4156-b2ae-6fa4bb88e693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990cbbcf-f52a-4ecf-8993-7f4d930e3b6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5970b6a9-1f7b-4e88-8d7a-25dae49e279b",
   "metadata": {},
   "source": [
    "## Evaluation on Test Data\n",
    "\n",
    "The model can then be used to evaluate how well it would perform on new data. Note that we shouldn't use the validation results because that was also used to determine how to build our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b5029d-9f41-47d3-970a-3541d81a33af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(test_examples, test_labels)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72363f8e-8fb0-4314-a167-83c6008b5f9c",
   "metadata": {},
   "source": [
    "## Other Pre-Built Models\n",
    "\n",
    "The [Hugging Face Hub](https://huggingface.co/models) has many models that have been pre-trained for you to use. You can access them using the `pipeline` function to get text analysis models, such as more advanced sentiment analysis than just using the VADER method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb35d10-8250-4c01-bf6b-e18a26a58463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "data = [\"I love you\", \"I hate you\"]\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095e382a-a190-4f3b-9b9d-b50a4a174647",
   "metadata": {},
   "source": [
    "We can apply this to our own data fairly easily, without needing to train anything. The text data just has to be in a list. For example, if we took our NYT abstracts and wanted to get the sentiments for each abstract, we could do that by giving a list of those abstracts as a list for the argument in `sentiment_pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62667ce5-c8cb-45c5-a5b4-a5fbaf3a453d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nyt_2021 = pd.read_csv('nyt_2021.csv').dropna()\n",
    "abstracts = nyt_2021.abstract.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c7d989-61dc-47f6-a2c7-b1d40bedf0c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "abstracts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4c8ffb-28bd-45d2-b433-3b9fe7fcb51d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only do 10 here for speed\n",
    "abstract_sentiments = sentiment_pipeline(abstracts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae44361-8a89-43a5-91e0-1ce99cb7664a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "abstract_sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684505cd-6bc2-4a6a-8cca-bca2fd36c5b9",
   "metadata": {},
   "source": [
    "## Other Types of Sentiment\n",
    "\n",
    "The nice thing about these models is that they are also pre-trained to do different types of sentiment analysis. For example, let's take the Distilbert-base-uncased-emotion model. This provides scores for emotions such as joy or anger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590b7962-541b-45a0-b596-3603dc1f760f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifier = pipeline(\"text-classification\",\n",
    "                      model='bhadresh-savani/distilbert-base-uncased-emotion', \n",
    "                      top_k=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03d9f39-8a99-49d7-961b-a93c0effe63c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction = classifier(\"I love using transformers. The best part is wide range of support and its easy to use\", )\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75731073-cf89-471d-8d04-0086482166c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction = classifier(abstracts[:10], )\n",
    "print(prediction[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
