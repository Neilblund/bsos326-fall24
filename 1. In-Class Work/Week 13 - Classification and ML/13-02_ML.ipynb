{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0660a349-b3ef-4082-b7a9-b05c5048461d",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf830732-d580-4896-b93d-9d16cc06f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "import re\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, classification_report, balanced_accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_validate\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "# included so I can add latex code\n",
    "from IPython.display import display, Math, Latex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2c0b3b-a306-4feb-b3aa-ec80cb2d467f",
   "metadata": {},
   "source": [
    "Importing the IMDB review data and applying the VADER sentiment analyzer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33054313-2776-4a32-aded-ff92b2c2ffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"imdb_reviews.csv\")\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# making the predictions dichotomous so they match the original labels\n",
    "reviews[\"vader\"] =  [sia.polarity_scores(i)['compound'] for i in reviews['text']]\n",
    "reviews[\"vader_prediction\"] =  pd.to_numeric(reviews['vader']>=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c7a3e8-2983-4a31-9e6e-b669276468ed",
   "metadata": {},
   "source": [
    "When we left last class, we were using a set of user reviews from IMDB to check the accuracy of the VADER sentiment analysis lexicon. We found that, overall, it did better than a random guess, but still had plenty of errors. But can we do better? \n",
    "\n",
    "Maybe! Instead of using a lexicon to determine if things are positive or negative, we can make a machine learning model that infers the terms that are associated with positive or negative documents automatically. In general, even fairly simple machine learning models can beat lexicon based methods. \n",
    "\n",
    "In **supervised machine learning**, we are focused on finding the relationship between a **label y** and **features x**.\n",
    "\n",
    "$$ y = f(x) $$\n",
    "\n",
    "\"Learning\" is finding a function f that minimizes future error in recovering y. \n",
    "\n",
    "For supervised learning, we must have a y variable that we know. That is, we need to have a the y variable in our dataset, so that we can build our model and use that model to predict y for future data.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note</b> in machine learning circles its common to use terms - like \"learning\", \"training\", \"memorization\" etc. - that can give the misleading impression that we're dealing with models that have some kind of human-like reasoning capcity. This is a useful metaphor because the idea of \"learning\" is a lot more intuitive than \"minimizing a loss function\", but what we're really doing here is fitting statistical models. Some of these models are very sophisticated, but none are conscious, they don't think, they don't have free will, and this is true even for the most cutting edge technologies. Just something to keep in mind. \n",
    "</div>\n",
    "\n",
    "\n",
    "We're going to compare the predictions from the VADER sentiment lexicon to another widely used machine learning model called a Naive Bayes classifier. \n",
    "\n",
    "\n",
    "The Naive Bayes model is *old* and extremely simple by modern standards, but its performs surprisingly well for simple classification tasks and is often used as a sort of baseline model for assessing other machine classifiers. It works by using Baye's Theorem to calculate the probability of each class (C_k in the formula below) given the predictors (X). \n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/52bd0ca5938da89d7f9bf388dc7edcbd546c118e)\n",
    "\n",
    "It's considered \"naive\" because it assumes (incorrectly!) that each word is independent of all other words. This is another one of those clearly wrong assumptions that still works. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe8e720-de78-48ac-bfd8-754e2e55098a",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "We'll start by setting up our pre-processing steps to create the bag-of-words model for our Naive Bayes classifier. But we won't actually apply it to our data just yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fa415e-4041-421b-bc63-a17a79343c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = reviews.text\n",
    "# tokenizer that splits words\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# word stemming\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "# english stop words\n",
    "# stem the stopwords to ensure they're removedb\n",
    "eng_stopwords = [tokenizer.tokenize(s)[0] for s in  stopwords.words('english')]\n",
    "\n",
    "def tokenize(text):   \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return [token for token in tokens if token not in eng_stopwords]\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer= \"word\", # unit of features are single words rather then phrases of words \n",
    "                             tokenizer = tokenize,\n",
    "                             ngram_range=(0,1), # Tokens are individual words for now\n",
    "                             strip_accents='unicode',\n",
    "                             max_df = 0.1, # maximum number of documents in which word j occurs. \n",
    "                             min_df = .0025 # minimum number of documents in which word j occurs. \n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035a7c27-a0a9-40c6-84df-6ef9e488a53c",
   "metadata": {},
   "source": [
    "# Train and Test sets\n",
    "\n",
    "One more caveat before we start: its extremely easy to make a model that \"predict\" data that's already part of the sample. A list of all U.S. presidential election winners in sequential order is a perfect predictor of all past elections, but its a terrible model for predicting the next one. Creating a model that predicts well inside a sample but poorly outside the sample is called \"overfitting\", and its a recurring problem for this kind of machine learning. In order to avoid that problem, we're going to randomly split our data into a training set that we'll feed to the model, and a validation (or training) set, that we'll use to assess our accuracy. \n",
    "\n",
    "The `train_test_split` function will do this for us. We'll use 80% of our data for training and the remaining 20% for validation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7ee57e-6a8c-4298-a755-fe07272d1be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(reviews, \n",
    "                                     test_size=0.20, # 20% of observations for validation\n",
    "                                     random_state = 999) # this is a random process, so you want to set a random seed! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0bd1d9-dc65-4a44-a434-4ced4c029121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have a training set and testing set:\n",
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4241b26a-5870-4158-ad79-af4864819543",
   "metadata": {},
   "source": [
    "Now we can apply the the vectorization function to make our bag of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d04036-2d72-4d1a-b564-0f46a2eb7826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the count vectorizer function on our training data\n",
    "X_train = vectorizer.fit_transform(df_train.text)\n",
    "\n",
    "# get the names of the features for future use\n",
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Note! We use transform instead of fit_transform to ensure that the vectorization function\n",
    "# doesn't update anyhing based on the testing data\n",
    "X_test = vectorizer.transform(df_test.text) \n",
    "\n",
    "# split the labels into train and test sets as well:\n",
    "y_train = df_train.label\n",
    "y_test = df_test.label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4ba108-77ce-40b4-9918-69ca5a66be7c",
   "metadata": {},
   "source": [
    "Now we'll fit our Naive Bayes model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64c5180-8b96-44ea-a10f-6f1ae01d5e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0724e9ba-75b4-455a-b243-3cfcb9f1b34e",
   "metadata": {},
   "source": [
    "Now we can use the trained model to predict data in the testing set, and compare the results in a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33be22b-542e-49d9-a0cd-a9e853a653f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = nb.predict(X_test)\n",
    "\n",
    "pd.crosstab(y_test, preds,  margins=True).rename_axis(index = 'Truth', columns='Predictions')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f044c58-6c9e-4c55-814f-4fd793513034",
   "metadata": {},
   "source": [
    "`scikit-learn` has a built in function that will allow us to get some additional information about our model performance. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba32873-150b-42bd-b47b-eb7b04e58347",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, preds, \n",
    "                            # add target_names to show labels in the report:\n",
    "                            target_names=['negative', 'positive']))\n",
    "\n",
    "# add cohen's kappa and balanced accuracy\n",
    "print(\"cohens kappa: \", cohen_kappa_score(y_test, preds))\n",
    "print(\"balanced accuracy: \", balanced_accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6034d882-53d0-4b98-a005-380022f3dd1e",
   "metadata": {},
   "source": [
    "Here's how we can intepret these metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcab4ae-ce2e-4934-9184-0c13b0983a7a",
   "metadata": {},
   "source": [
    "| **metric**                  | Description                                                                                                                               |\n",
    "|-----------------------------|-------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **accuracy** | % of predictions that are accurate                                                                                                        |\n",
    "| **recall**                  | % of actually positive reviews that were correctly classified as positive                                                                 |\n",
    "| **precision**               | % of predicted positive reviews that were actually positive                                                                               |\n",
    "| **f-1**                     | Harmonic mean of precision and recall. Used as an overall measure of model performance. The maximum score is 1. Scores above .5 are poor. |\n",
    "| **Cohen's Kappa**           | Measures how well the model performs relative to a model based on the marginal probabilities of each class. Higher is better.             |\n",
    "| **Balanced Accuracy**       | Accuracy score after accounting for imbalance between each class                                                                          |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1afd3c1-0e38-4375-a543-3f07dd9ebe3a",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> Question 1: Create a confusion matrix and classification report for the `vader_prediction` column in the validation data  <span/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3a18d0-4e47-4b56-82f2-50e5b2cfc746",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4154ec-a730-45ab-aa50-668f052c0499",
   "metadata": {},
   "source": [
    "So how does the naive bayes model perform relative to VADER? More importantly why is there a difference? Primarily, this probably comes down to the differences in context: there are a lot of terms that indicate negative views in the IMDB corpus that probably wouldn't indicate negative views in other contexts. We can get a sense of this by extracting some of the most important features from the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691903f5-e147-4bf9-8928-d6a698fd4948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the probability of positive and negative classes\n",
    "prob_neg = df_train['label'].value_counts(normalize=True)[0]\n",
    "prob_pos = df_train['label'].value_counts(normalize=True)[1]\n",
    "# making a data frame with the \n",
    "df_nbf = pd.DataFrame()\n",
    "df_nbf.index = features\n",
    "vals= np.e**(nb.feature_log_prob_[0, :])\n",
    "# np.e exponentiates the logged odds, so this turns them back into probabilities \n",
    "df_nbf['pos'] = np.e**(nb.feature_log_prob_[1, :]) # log probability for negative class\n",
    "df_nbf['neg'] = np.e**(nb.feature_log_prob_[0, :]) # log probability for positive class\n",
    "# terms with the highest ratio of association with one class\n",
    "# p(positive|word)/p(negative|word) * (p(positive)/p(negative))\n",
    "df_nbf['odds_positive'] = (df_nbf['pos']/df_nbf['neg'])*(prob_pos /prob_neg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1ee653-fb94-42d3-868a-5d9fb812fff0",
   "metadata": {},
   "source": [
    "Now we can look at some features associated with positive or negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d8ef98-fb8c-4d72-ad81-a5fec2dc6aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nbf.sort_values('odds_positive',ascending=False)['odds_positive']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee3fc62-7f51-4b25-a43a-81f5e8f2b432",
   "metadata": {},
   "source": [
    "And with a little reshaping, we can plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2167e9-a218-47d4-b537-4cb5424cbfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top = df_nbf.sort_values('odds_positive',ascending=False).reset_index()\n",
    "top_bottom = pd.concat([top.iloc[:15], top.iloc[-15:]])\n",
    "ax = sns.barplot(data=top_bottom,\n",
    "                 y= 'index',    \n",
    "                 hue='index',\n",
    "                x=np.log(top_bottom['odds_positive']),dodge=False, palette='turbo')\n",
    "ax.set(xlabel='Strength of association with positive\\n vs. negative reviews', ylabel='term')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049d5c72-457d-4cf4-aeb7-cf0227de976c",
   "metadata": {},
   "source": [
    "The results here should give you a rough idea of how and why the Naive Bayes model is able to outperform VADER: there are a number of terms - particularly the names of actors and directors - that are strongly associated with negative or positive reviews in this corpus that wouldn't really be seen as indicating negativity elsewhere. It might also give you a sense of one of the key risks of using machine learning approaches: its easy to unintentionally fit models that make inferences that might reflect biases - including cultural, racial, or gender biases - that we don't want to perpetuate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13607b83-2109-4826-88a7-6919c96d2d1c",
   "metadata": {},
   "source": [
    "# Improving the processing\n",
    "\n",
    "Naive Bayes is generally considered a baseline model for classification, but sometimes we can improve our results with some additional pre-processing. We've already explored some options for pre-processing text data in previous classes: we can do things like lemmatization, using n-grams, or applying a weighting scheme like tf-idf can make predictive models more effective. We skipped all of those steps when building this model, so. before we move on to a more complicated method, maybe we should see if we can do better by cleaning the data up a little.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a95ca6-0107-407f-a736-089074175375",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> Question 2: Try changing the pre-processing steps for the reviews data. What steps you choose here is up to you. Fit a new naive bayes classifer on the processed data, and assess your results on the held-out data. <span/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7185f48-1c6b-4204-84e4-280c14d91857",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771f9786-01ae-4c76-adeb-f15d84c65d15",
   "metadata": {},
   "source": [
    "## Fitting a different model\n",
    "\n",
    "Can we do better with a better model? The Naive Bayes classifier, after all, fails to account for correlations between predictors and can't really handle interactive relationships. We can compare our results to a support vector machine to see if we get better results. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1587d2d1-9202-4efd-926a-153978f9c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC()\n",
    "model.fit(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50444d92-7f66-40a3-8426-417fff588bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "print(classification_report(y_test, pred, \n",
    "                            # add target_names to show labels in the report:\n",
    "                            target_names=['negative', 'positive']))\n",
    "\n",
    "# add cohen's kappa and balanced accuracy\n",
    "print(\"cohens kappa: \", cohen_kappa_score(y_test, preds))\n",
    "print(\"balanced accuracy: \", balanced_accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965725af-98b5-47e0-8505-28d5c4a7ee3c",
   "metadata": {},
   "source": [
    "Next week we'll look at a newer and more flexible strategy for classifying texts (and other things) using neural network models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe9f579-7368-4345-b918-533cb443b65f",
   "metadata": {},
   "source": [
    "## K-fold cross validation\n",
    "\n",
    "Although its unlikely, given how many test examples we have, we might want to make sure that our performance with this model isn't just a function of random chance, and we also want to avoid over-fitting to a single test data set. To avoid this problem, we'll often use k-fold cross validation. In K-fold cross validation, we separate the data into \"K\" equally sized groups, and then loop through the folds using each one as the validation data and all of the remaining observations as training data. Scikit-learn has an easy method for running k-fold cross validation and getting some overall metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3e90d2-cc95-42dc-8d79-2daa9f75582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = vectorizer.fit_transform(reviews.text)\n",
    "nbayes = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9084cf16-6ce0-4641-8ff2-9d30f8a3bd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val = cross_validate(nbayes, \n",
    "               all_reviews, \n",
    "               reviews.label, \n",
    "               cv=40,\n",
    "               scoring =['f1', 'balanced_accuracy']\n",
    "              \n",
    "              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c3d50f-c597-40b9-9f4e-edb72d06c3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cross_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
