{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09ae41cb-e984-49f9-a885-3c2b70191915",
   "metadata": {},
   "source": [
    "# Topic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5231973-2244-4a11-b880-4871be3a06ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "import re\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import string\n",
    "import time\n",
    "\n",
    "from IPython.display import display, HTML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da31c466-353c-4ce9-9427-e0e3f4138d81",
   "metadata": {},
   "source": [
    "Also install `pyLDAvis` and import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6c2e89-942f-4177-ba74-77c091503163",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyLDAvis\n",
    "import pyLDAvis.lda_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922ebb54-5541-4fb3-9b4c-9c0f03429a9a",
   "metadata": {},
   "source": [
    "We'll start by reading in some example data. This .csv contains articles scraped from CNN and Fox News from around 2020. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6078f3df-3dd1-4321-aa43-e507ed2441c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "articles = pd.read_csv('https://github.com/Neilblund/APAN/raw/main/news_sample.csv')\n",
    "# stripping some excess whitespace\n",
    "articles['headline'] = articles.headline.str.strip()\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d7a6de-485b-4cc1-8dba-66d4403791d8",
   "metadata": {},
   "source": [
    "Example of using some HTML markup to format hyperlinks (we'll use this later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbfc7d1-fcc1-40d1-af1b-d930ecc482ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['hyperlink']=articles.apply(axis=1, func = lambda x: f'<a href={x.url}>{x.headline}</a>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02070ea-7929-4a39-ba80-122ce1996f3b",
   "metadata": {},
   "source": [
    "## Text Analysis - Topic Modeling\n",
    "\n",
    "Suppose we have a corpus of text data that we want to understand more about. We might want to get a sense for what was covered in the news in 2020. We could read all of them ... but that would take a really long time, and it would not be feasible to do that in any reasonable amount of time. Also, it'd be hard for us to process that amount of data manually. \n",
    "\n",
    "Instead, we can use a technique called **topic modeling** using **Latent Dirichlet Allocation** (LDA). This will allow us to automatically generate topics that describe the documents within a corpus, as well as determine which documents fit into which topics. \n",
    "\n",
    "To do this, though, we first need to convert texts to a \"bag-of-words\" representation and then into a format called a document-term-matrix.\n",
    "\n",
    "In the last class, we converted texts to a bag-of-words by doing the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a587dd4-62fb-4da1-a8e8-72e84b78dd69",
   "metadata": {},
   "source": [
    "1. **Tokenization** splits texts into smaller units. In the current case, this will be individual words. But it could also be sentences, paragraphs or \"n-gram\" (groups of 1, 2, 3... words)\n",
    "2. **Stopword Removal** removing common terms like \"a\", \"and\", \"the\" that are grammatically useful but uninformative for many text models\n",
    "3. **Normalization** combining terms that are more-or-less equivalent by doing things like converting words to lower-case or removing word endings through stemming or lemmatization\n",
    "\n",
    "\n",
    "(In some cases we may change the ordering of these steps, or we might do some cleaning and normalization and then decide, based on a closer look at our results, that we need to go back and do some additional cleaning. Our end goal, however, is generally to a representation of a text that is simple enough without sacrificing too much nuance or complexity\n",
    "\n",
    "To make a document-term-matrix we're going to conver the bag of words to format with one row per document, one column for each word in the entire corpus, and a count of word \"j\" in document \"i\" in each cell. So:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da191aaf-2424-44db-85dd-1561a4e7cfbe",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\"><thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\">See</th>\n",
    "    <th class=\"tg-0pky\">Spot</th>\n",
    "    <th class=\"tg-0pky\">Run</th>\n",
    "    <th class=\"tg-0pky\">Fast</th>\n",
    "  </tr></thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">See Spot run.</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Spot runs fast</td>\n",
    "    <td class=\"tg-0pky\">0</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Run Spot run.</td>\n",
    "    <td class=\"tg-0pky\">0</td>\n",
    "    <td class=\"tg-0pky\">1</td>\n",
    "    <td class=\"tg-0pky\">2</td>\n",
    "    <td class=\"tg-0pky\">0</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "(For efficiency's sake, document term matrices like this will typically be represented as \"sparse matrices\", which only index the non-zero elements of each cell, but at least conceptually you should still think of a document term matrix as being structured like the example above)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d36900-b6fa-4bf6-af9f-55f398078fc5",
   "metadata": {},
   "source": [
    "The `scikit-learn` package has a `CountVectorizer` function that will let us do all of this processing in a single step, so we'll use that to create a document term matrix. We'll start by lowercasing all of our text, then we'll create the Count Vectorizer and apply it to the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ca92b6-bcf8-43b4-b996-989de5d17fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case and drop empty\n",
    "\n",
    "text = articles.text.str.lower().reset_index().text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8188a01b-56d6-4b43-a82b-f352547c27b7",
   "metadata": {},
   "source": [
    "In addition to stemming and stopword removal, you might notice we also set values for `max_df` and `min_df`. `max_df=0.1` means that we remove all terms that occur in more than 10% of the documents. This can be a useful way to remove common terms that might not be captured by a standard stopword list, and it has the same basic motivation: we want to remove terms that are not really informative. Setting `min_df=.0025` has the effect of removing terms that occur in less than 0.25% of documents, so it gets rid of rare terms which - although they might be informative - don't show up in the corpus enough times for our topic model to really \"learn\" anything about them. Fiddling with both of these parameters may change your results for better or worse, so it may be worth experimenting with different values here if you're not satisfied with the results of your topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe175a8a-dd69-42bb-ae73-3c0edbf2cfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer that splits words\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# word stemming\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "# english stop words\n",
    "# stem the stopwords to ensure they're removedb\n",
    "eng_stopwords = [tokenize(s)[0] for s in  stopwords.words('english')]\n",
    "\n",
    "def tokenize(text):   \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return [stemmer.stem(token) for token in tokens if token not in eng_stopwords]\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer= \"word\", # unit of features are single words rather then phrases of words \n",
    "                             tokenizer = tokenize,\n",
    "                             ngram_range=(0,1), # Tokens are individual words for now\n",
    "                             strip_accents='unicode',\n",
    "                             max_df = 0.1, # maximum number of documents in which word j occurs. \n",
    "                             min_df = .0025 # minimum number of documents in which word j occurs. \n",
    "                            )\n",
    "\n",
    "\n",
    "\n",
    "bag_of_words = vectorizer.fit_transform(text) #transform our corpus into a bag of words \n",
    "features = vectorizer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d61b38-eed5-4dca-a167-61bf70cc5eb0",
   "metadata": {},
   "source": [
    "Our resulting object is a matrix with one row per document and one column per term. We'll feed this into subsequent text analysis models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed95519-8bcb-45f6-b85b-13670a35d910",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc4b70d2-3bb5-4fe6-b808-375950b73aee",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation\n",
    "\n",
    "Next, we fit the Latent Dirichlet Allocation (LDA) model. LDA is a statistical model that generates groups based on similarities. This is an example of an **unsupervised machine learning model**. That is, we don't have any sort of outcome variable -- we just need to have some very rough idea regarding the number of topics in our corpus, and the LDA model will identify them for us. \n",
    "\n",
    "\n",
    "#### The Dirichlet Distribution\n",
    "\n",
    "A dirichlet distribution is a probability distribution that can be used to describe the probability distribution for multivariate outcomes. For instance, if we were manufacturing a bunch of unbalanced dice, I could use a random dirichlet distribution to model the probability of each face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a279e97-02f6-4eb7-ad30-025d99746484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 random unbalanced dice: \n",
    "rng = np.random.default_rng()\n",
    "s = pd.DataFrame(rng.dirichlet((1,1,1,1,1,1), 10), columns = range(1, 7, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6821ef-03d0-4496-8337-e45ebba1de2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s.plot.barh(stacked=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0073d9a5-b05c-4e17-843b-d024770f8e72",
   "metadata": {},
   "source": [
    "In LDA, we assume that documents are created by sampling from two dirichlet distributions:\n",
    "\n",
    "\n",
    "- The **Topic-word distribution** (sometimes called phi or beta)  models the probability of any word occuring in a single topic. A topic related to Covid-19 might have a high probability of terms like \"Fauci\", \"mask\", or vaccine\". A topic related to the election might have a high probabiltiy of \"vote\", \"caucus\" or \"turnout\"\n",
    "- The **Document-Topic distributions** (theta) models the probability of drawing one of K topics within a given document. So a document might be drawn 75% from the Covid-19 topic, 20% from the election topic, and 5% from some other random topic.\n",
    "\n",
    "\n",
    "\n",
    "LDA assumes a generative model where each article is written randomly sampling topics from a document's topic distribution, and then randomly sampling each word from the chosen topic distribution. So I might write a document where I drew 75% of my words from a \"Covid-topic\" and 20% from a \"election topic\" and another 5% from some other random topic. \n",
    "\n",
    "Again: keep in mind that the actual topics here are *latent*: we'll never observe them. Instead, we assume they exist and then try to infer their parameters from the distribution of words in each document in the corpus. If our model works, then we should be able to identify topics from our LDA model that have high probabilities of a bunch of semantically related terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6022a7d1-6db3-446b-86ea-8a01b0d6f5c6",
   "metadata": {},
   "source": [
    "## Fitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48699a9a-bd9d-456d-8640-868ba6e20b6e",
   "metadata": {},
   "source": [
    "Let's try fitting an LDA model. We need to choose the number of topics ourselves. We'll assume there are 20 topics here. We'll also want to set a `random_state` value. Multiple LDA models with the same number of topics and the same data should find similar topics, but even then the ordering of those topics is arbitrary, so setting the `random_state` variable will ensure that we can replicate our results.\n",
    "\n",
    "\n",
    "\n",
    "We first create a LatentDirichletAllocation object, then fit it using our corpus bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccdfe14-efa5-46fc-aeeb-fbe4afb79424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LDA model object\n",
    "k =15\n",
    "lda = LatentDirichletAllocation(n_components = k, # number of topics. Try different numbers here to see what works best. Usually somewhere between 20 - 100\n",
    "                                random_state = 123, # random number seed. You can use any number here, but its important to include so you can replicate analysis\n",
    "                                doc_topic_prior = 1/k) \n",
    "\n",
    "# Fit using data (bag_of_words)\n",
    "doctopic = lda.fit_transform(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361ab8b1-90da-4c46-aae2-b4ce58c2be43",
   "metadata": {},
   "source": [
    "## Interpreting the results\n",
    "A lot of the complexity of using LDA lies in intepreting the topics themselves. The simplest way to do this is by looking at the most probable words for each topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2424e8ab-94ae-4df8-b303-298947451f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the top keywords in each topic\n",
    "\n",
    "n_terms = 10\n",
    "ls_keywords = []\n",
    "ls_freqs = []\n",
    "topic_id = []\n",
    "\n",
    "for i,topic in enumerate(lda.components_):\n",
    "    # Sorting and finding top keywords\n",
    "    word_idx = np.argsort(topic)[::-1][:n_terms]\n",
    "    freqs = list(np.sort(topic)[::-1][:n_terms])\n",
    "    keywords = [features[i] for i in word_idx]\n",
    "    \n",
    "    # Saving keywords and frequencies for later\n",
    "    ls_keywords = ls_keywords + keywords\n",
    "    ls_freqs = ls_freqs + freqs\n",
    "    topic_id = topic_id + [i+1] * n_terms\n",
    "    \n",
    "\n",
    "    # Printing top keywords for each topic\n",
    "    print(i, ', '.join(keywords))\n",
    "\n",
    "top_words_df = pd.DataFrame({'keywords':ls_keywords, 'frequency':ls_freqs, 'topic_id':topic_id})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040055c8-aa77-49b6-8f08-c5f5c6ea2996",
   "metadata": {},
   "source": [
    "<b style=\"color:red;\"> Question 1: Wrap the code above in a function that takes `n_terms` and `lda` as arguments and returns a data frame with the top n terms for each topic in descending order of frequency. Try running the same function with a few different values for `n_terms`</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d235e3-60ae-4ac8-99c6-b5f9c1efbb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2710c-a995-415c-b509-fad75ea4a32a",
   "metadata": {},
   "source": [
    "We can also plot terms by frequency within each topic (although this may get unwieldy for models with a larger number of components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20514c9e-fd24-492b-a66d-a5fbce73746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(top_words_df, x = 'frequency', y = 'keywords', col = 'topic_id', kind = 'bar', sharey = False, col_wrap=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7202ba-2d61-453e-9321-d10ee3353b94",
   "metadata": {},
   "source": [
    "In many cases, an interactive visualization can make it easier to identify topics. The LDAvis package provides an easy way to create an interactive HTML file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2739f94c-e284-4691-b82e-a2799620a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel = pyLDAvis.lda_model.prepare(lda, bag_of_words, vectorizer, mds='tsne', sort_topics=False, n_jobs = -1)\n",
    "word_info = panel.topic_info\n",
    "\n",
    "#To save panel in html\n",
    "pyLDAvis.save_html(panel, 'panel.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1a2080-5b9e-4f2c-bd60-b73d1ffe32c2",
   "metadata": {},
   "source": [
    "In the left panel if the display below, you can see each topic scaled by its overall frequency in the corpus. The relative positions of each topic indicates how distinct they are, so that topics that are further apart should share fewer terms. The plot on the right will display top terms for each topic. Instead of using the probability of each term, the displayed in this visualization are ranked according to a metric that accounts for how specific each term is to each topic. In some cases, this can be a better way of identifying the concept each topic represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6723f748-aa2e-4288-adb8-a2f3dce85682",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('panel.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b99b451-f576-4892-aa72-afb5b4acd9c8",
   "metadata": {},
   "source": [
    "<b style=\"color:red;\"> Question 2: Using the information we've gathered so far, see if you can assign a short label to each topic in the LDA model. Replace the generic labels in `label_map` below with some descriptive topic IDs and then recreate the catplot object from the previous section</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51678fb5-5758-4934-9c9b-54d9688bd521",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    0: 'topic 1',\n",
    "    1: 'topic 2',\n",
    "    2: 'topic 3',\n",
    "    3: 'topic 4',\n",
    "    4: 'topic 5',\n",
    "    5: 'topic 6',\n",
    "    6: 'topic 7',\n",
    "    7: 'topic 8',\n",
    "    8: 'topic 9',\n",
    "    9: 'topic 10',\n",
    "    10: 'topic 11',\n",
    "    11: 'topic 12',\n",
    "    12: 'topic 13',\n",
    "    13: 'topic 14',\n",
    "    14: 'topic 15',\n",
    "}\n",
    "# map the labels\n",
    "top_words_df['topic_label'] = top_words_df['topic_id'].map(label_map)\n",
    "\n",
    "# recreate the catplot object:\n",
    "sns.catplot(top_words_df, x = 'frequency', y = 'keywords', col = 'topic_label', kind = 'bar', sharey = False, col_wrap=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b189cd2-086b-47a5-8770-8e89b7bce3d4",
   "metadata": {},
   "source": [
    "## Identifying Document Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd19c5d3-dc21-4f15-b368-dcad61d18698",
   "metadata": {},
   "source": [
    "We can link the topic memberships in `doctopic` back to the original documents so that we can see which documents are getting categorized into which topics. This does mean we need to match the column name to the appropriate topic title, so you can also adjust the column titles with an appropriate one based on what you determined that topic to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1070fd35-6487-4343-b282-4f10f2ab00f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_memberships = pd.DataFrame(doctopic)\n",
    "topic_memberships.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55baf222-a54b-47a8-b830-f158760b2cb3",
   "metadata": {},
   "source": [
    "Each row in this result represents one of the documents from our original data, and each column represents a topic. We can make things a little easier to interpret by appending some information about each article as additional columns onto this data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9645f87c-2498-4e89-ab49-d68991383ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_memberships['text'] = text\n",
    "topic_memberships['source'] = articles.source\n",
    "topic_memberships['headline'] =articles.headline\n",
    "topic_memberships['url'] = articles.url\n",
    "topic_memberships['hyperlink'] = articles.hyperlink\n",
    "\n",
    "topic_memberships.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0203533a-642f-4254-9b96-f5b56b8ae6e0",
   "metadata": {},
   "source": [
    "<b style=\"color:red;\"> Question 3: Identify the top 5 articles most strongly associated with topic 6</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae50134-9b3f-4253-b4f7-230345add0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09033833-8832-4f84-a5eb-9a450db8583e",
   "metadata": {},
   "source": [
    "<b style=\"color:red;\"> Question 4: Which topics, if any, occurred more often in Fox News stories vs. CNN?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec8603a-be9c-4b82-b8df-674d96cc4311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f96b71-bafb-47e2-aca7-60b0f35ab19b",
   "metadata": {},
   "source": [
    "# Making a styled table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133f151d-67c1-48b2-aa45-13bf46c70a2e",
   "metadata": {},
   "source": [
    "Since we included a hyperlink and an article title in our original data frame, we can make a styled table that includes a formatted link for the topic articles in each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e4195a-bd94-4d11-b8e2-327285887d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_terms = 10\n",
    "n_docs = 3\n",
    "top_documents = []\n",
    "top_index = topic_memberships.columns.values.tolist()\n",
    "for i in range(k):\n",
    "    top_n_documents =  topic_memberships.sort_values(i, ascending=False).head()\n",
    "    terms={ 'topic' : i,\n",
    "           'mean proportion' : np.mean(topic_memberships[i]),\n",
    "        'docs' : '<br>'.join([i for i in top_n_documents['hyperlink'].to_list()[:n_docs]]),\n",
    "        'terms' : ', '.join([features[j] for j in np.argsort(lda.components_[i])[::-1][:n_terms]]) \n",
    "    }\n",
    "    top_documents.append(terms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b076abba-70f8-4e0c-9037-cde210720ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(top_documents).sort_values(['mean proportion'], ascending=False).style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f412fb79-7ee9-4edd-98dc-3b372f6602ae",
   "metadata": {},
   "source": [
    "<b style=\"color:red;\"> Question 5: Try re-running the model with a larger number of topics. Compare your results. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa3565f-9228-4a2e-92a2-8ab85e82fb57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
