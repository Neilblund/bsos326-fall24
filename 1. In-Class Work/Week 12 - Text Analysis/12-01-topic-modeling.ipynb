{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Topic Modeling with Latent Dirichlet Allocation\n",
    "\n",
    "Use **Code** cells to write and run any code you need to answer the question and **Markdown** cells to write out answers in words. After you are finished with the assignment, remember to download it as an **HTML file** and submit it in **ELMS**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "import re\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis - Topic Modeling\n",
    "\n",
    "Suppose we have a corpus of text data that we want to understand more about. For example, let's say we have all articles that were published in the New York Times in 2021. We might want to get a sense for what was covered in the news in 2021. We could read all of them ... but that would take a really long time, and it would not be feasible to do that in any reasonable amount of time. Also, it'd be hard for us to process that amount of data manually. \n",
    "\n",
    "Instead, we can use a technique called **topic modeling** using **Latent Dirichlet Allocation**. This will allow us to automatically generate topics that describe the documents within a corpus, as well as determine which documents fit into which topics. \n",
    "\n",
    "To do this, though, we first need to get the data into a matrix form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nyt_2021 = pd.read_csv('nyt_2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nyt_2021.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall: Text Data into a Matrix Format\n",
    "\n",
    "We're going to treat each token as a variable and each document as an observation. So, in the case of NYT Article abstracts, we will treat individual article abstract as an observation. There will be as many columns as there are unique tokens in the overall corpus (so there will be many many variables!). The dataset that we end up with will looking something like this:\n",
    "\n",
    "|document ID|about|america|author|ask|...|\n",
    "|-|-|-|-|-|-|\n",
    "|1|0|0|0|0|...|\n",
    "|2|0|1|0|0|...|\n",
    "|3|0|0|3|0|...|\n",
    "|4|1|0|0|0|...|\n",
    "|5|0|0|0|2|...|\n",
    "|...|...|...|...|...|...|\n",
    "\n",
    "To convert our abstracts into this format, we first take a Series of the abstracts with everything lowercased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "abstracts = nyt_2021.abstract.str.lower().reset_index().abstract.dropna()\n",
    "abstracts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a `tokenize` function that does the tokenizing and temming steps that we had done before. This is a function that we will need to provide to `CountVectorizer` below instead of using directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return [stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## CountVectorizer\n",
    "\n",
    "We can apply this to each abstract in our corpus using `CountVectorizer`. This will not only do the tokenizing, but it will also count any duplicates of words and create a matrix that contains the frequency of each word. This will be quite a large matrix (number of columns will be number of unique words), so it outputs the data as a sparse matrix.\n",
    "\n",
    "We will first create the `vectorizer` object (you can think of this like a model object), and then fit it with our abstracts. This should give us back our overall corpus bag of words, as well as a list of features (that is, the unique words in all the abstracts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize stop words to match\n",
    "eng_stopwords = [tokenize(s)[0] for s in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer= \"word\", # unit of features are single words rather then phrases of words \n",
    "                             tokenizer=tokenize, # function to create tokens\n",
    "                             ngram_range=(0,1), # Tokens are individual words for now\n",
    "                             strip_accents='unicode',\n",
    "                             stop_words= eng_stopwords,\n",
    "                             min_df = 0.01,\n",
    "                             max_df = 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have created the vectorizer, we can use it to transform our abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bag_of_words = vectorizer.fit_transform(abstracts) #transform our corpus into a bag of words \n",
    "features = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since this can be quite large, it will be stored as a sparse matrix. That is, it only stores information about which rows and columns have non-zero values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation\n",
    "\n",
    "Next, we fit the Latent Dirichlet Allocation (LDA) model. LDA is a statistical model that generates groups based on similarities. This is an example of an **unsupervised machine learning model**. That is, we don't have any sort of outcome variable -- we're just trying to group the abstracts into rough categories.\n",
    "\n",
    "![LDA Topics](lda1.png)\n",
    "\n",
    "The model does this by associating words with topics and making documents parts of topics based on the words that are part of that document. The more words that a document has relating to a topic, the more likely it is to be about that topic. This is fit using an iterative process, and the topics are defined by their **top words.**\n",
    "\n",
    "![LDA Words](lda2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try fitting an LDA model. We first create a `LatentDirichletAllocation` object, then fit it using our corpus bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LDA model object\n",
    "lda = LatentDirichletAllocation(n_components = 5, learning_method='online') \n",
    "\n",
    "# Fit using data (bag_of_words)\n",
    "doctopic = lda.fit_transform(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `lda.fit_transform` fits our model with our data (`bag_of_words`). Now, we just need to access it to see the results. One way to do this is by looking at the top words within each topic to see what each of the topics were about. \n",
    "\n",
    "In order to get the word membership within topics, we can use `lda.components_`. You can think about this as the number of times each word appeared within each topic (with some normalization). Each element of `lda.components_` is a list representing a topic and containing the word frequencies for each word. So, we can loop through each topic and sort by the most frequent words to print out. \n",
    "\n",
    "The code to do this is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the top keywords in each topic\n",
    "ls_keywords = []\n",
    "ls_freqs = []\n",
    "topic_id = []\n",
    "\n",
    "for i,topic in enumerate(lda.components_):\n",
    "    # Sorting and finding top keywords\n",
    "    word_idx = np.argsort(topic)[::-1][:10]\n",
    "    freqs = list(np.sort(topic)[::-1][:10])\n",
    "    keywords = [features[i] for i in word_idx]\n",
    "    \n",
    "    # Saving keywords and frequencies for later\n",
    "    ls_keywords = ls_keywords + keywords\n",
    "    ls_freqs = ls_freqs + freqs\n",
    "    topic_id = topic_id + [i+1] * 10\n",
    "\n",
    "    # Printing top keywords for each topic\n",
    "    print(i, ', '.join(keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color ='red'>**Question 1: Copy and paste the code in the cell above, then change it to display the top 20 most frequent words. What are the topics that you see?**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Word Frequencies within Topics\n",
    "\n",
    "We were able to get the top words within each topic, but we can also look at the frequencies within each of the words to get a better idea of what words were most frequent within each topic. This can also help us identify any words that might end up being context-specific stop words.\n",
    "\n",
    "To do this, we'll first make a DataFrame using the keywords and frequencies that we saved, as well as the topic IDs. Then, we can display them all using a bar graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_words_df = pd.DataFrame({'keywords':ls_keywords, 'frequency':ls_freqs, 'topic_id':topic_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.catplot(top_words_df, x = 'frequency', y = 'keywords', col = 'topic_id', kind = 'bar', sharey = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color ='red'>**Question 2: What are some context-specific stop words that you can identify from these bar graphs? Add those stop words to the existing `eng_stopwords` and call the new list `full_stopwords`.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_stopwords = eng_stopwords + []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Memberships\n",
    "\n",
    "We can link the topic memberships in `doctopic` back to the original documents so that we can see which documents are getting categorized into which topics. To do this, we just make a DataFrame with the `doctopic` object, then add the `abstracts`. This does mean we need to match the column name to the appropriate topic title, so you can also adjust the column titles with an appropriate one based on what you determined that topic to be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_memberships = pd.DataFrame(doctopic)\n",
    "topic_memberships['abstract'] = abstracts\n",
    "topic_memberships.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color ='red'>**Question 3: Based on the words observed earlier, rename the column names in `topic_memberships`. What topic does the first abstract seem to be about? Does this make sense?**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating the Process\n",
    "\n",
    "We've fit one LDA model. This gives us some insights into the data, but we shouldn't just stop there. We can refine this model and try to see if we can get more out of it by changing some things, like adding new stop words or changing the number of topics we think there might be. \n",
    "\n",
    "Let's try one example of an update we can make. Since we've defined a new stop word list, we can use that instead of the original `eng_stopwords`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer= \"word\", # unit of features are single words rather then phrases of words \n",
    "                             tokenizer=tokenize, # function to create tokens\n",
    "                             ngram_range=(0,1), # Tokens are individual words for now\n",
    "                             strip_accents='unicode',\n",
    "                             stop_words= full_stopwords,\n",
    "                             min_df = 0.01,\n",
    "                             max_df = 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LDA model object\n",
    "lda = LatentDirichletAllocation(n_components = 5, learning_method='online') \n",
    "\n",
    "# Fit using data (bag_of_words)\n",
    "doctopic = lda.fit_transform( bag_of_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the top keywords in each topic\n",
    "ls_keywords = []\n",
    "ls_freqs = []\n",
    "topic_id = []\n",
    "\n",
    "for i,topic in enumerate(lda.components_):\n",
    "    # Sorting and finding top keywords\n",
    "    word_idx = np.argsort(topic)[::-1][:10]\n",
    "    freqs = list(np.sort(topic)[::-1][:10])\n",
    "    keywords = [features[i] for i in word_idx]\n",
    "    \n",
    "    # Saving keywords and frequencies for later\n",
    "    ls_keywords = ls_keywords + keywords\n",
    "    ls_freqs = ls_freqs + freqs\n",
    "    topic_id = topic_id + [i+1] * 10\n",
    "\n",
    "    # Printing top keywords for each topic\n",
    "    print(i, ', '.join(keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color ='red'>**Question 4: Try running the code above again using a different number of topics. What are the topics that show up when you do this?**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
