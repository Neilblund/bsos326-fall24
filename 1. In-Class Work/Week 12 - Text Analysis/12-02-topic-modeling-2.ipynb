{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Refining Latent Dirichlet Allocation Models\n",
    "\n",
    "Use **Code** cells to write and run any code you need to answer the question and **Markdown** cells to write out answers in words. After you are finished with the assignment, remember to download it as an **HTML file** and submit it in **ELMS**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "import re\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data - NYT 2021 Archive\n",
    "\n",
    "As before, let's bring in all articles from the NYT Archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nyt_2021 = pd.read_csv('nyt_2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nyt_2021.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we do the setup for the pre-processing steps such as tokenizing, stemming, and removing stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "abstracts = nyt_2021.abstract.str.lower().reset_index().abstract.dropna()\n",
    "abstracts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return [stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## First Attempt with LDA\n",
    "\n",
    "We can apply the pre-processing to each abstract in our corpus using `CountVectorizer`. This will not only do the tokenizing, but it will also count any duplicates of words and create a matrix that contains the frequency of each word. This will be quite a large matrix (number of columns will be number of unique words), so it outputs the data as a sparse matrix.\n",
    "\n",
    "We will first create the `vectorizer` object (you can think of this like a model object), and then fit it with our abstracts. This should give us back our overall corpus bag of words, as well as a list of features (that is, the unique words in all the abstracts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize stop words to match\n",
    "eng_stopwords = [tokenize(s)[0] for s in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer= \"word\", # unit of features are single words rather then phrases of words \n",
    "                             tokenizer=tokenize, # function to create tokens\n",
    "                             ngram_range=(0,1), # Tokens are individual words for now\n",
    "                             strip_accents='unicode',\n",
    "                             stop_words= eng_stopwords,\n",
    "                             min_df = 0.01,\n",
    "                             max_df = 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have created the vectorizer, we can use it to transform our abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bag_of_words = vectorizer.fit_transform(abstracts) #transform our corpus into a bag of words \n",
    "features = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting LDA model\n",
    "\n",
    "# Create LDA model object\n",
    "lda = LatentDirichletAllocation(n_components = 5, learning_method='online') \n",
    "\n",
    "# Fit using data (bag_of_words)\n",
    "doctopic = lda.fit_transform( bag_of_words )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `lda.fit_transform` fits our model with our data (`bag_of_words`). Now, we just need to access it. We'll define a function that does this so that it is easier to do for later cases as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_keywords(lda, nwords, verbose = True):\n",
    "    '''\n",
    "    Display the top words within each topic after running Latent Dirichlet Allocation.\n",
    "    \n",
    "    Arguments:\n",
    "        lda: lda object\n",
    "        nwords: number of words to display for each topic\n",
    "    Returns:\n",
    "        A DataFrame containing keywords, frequencies, and topic ID.\n",
    "    '''\n",
    "    # Displaying the top keywords in each topic\n",
    "    ls_keywords = []\n",
    "    ls_freqs = []\n",
    "    topic_id = []\n",
    "\n",
    "    for i,topic in enumerate(lda.components_):\n",
    "        # Sorting and finding top keywords\n",
    "        word_idx = np.argsort(topic)[::-1][:nwords]\n",
    "        freqs = list(np.sort(topic)[::-1][:nwords])\n",
    "        keywords = [features[i] for i in word_idx]\n",
    "\n",
    "        # Saving keywords and frequencies for later\n",
    "        ls_keywords = ls_keywords + keywords\n",
    "        ls_freqs = ls_freqs + freqs\n",
    "        topic_id = topic_id + [i+1] * nwords\n",
    "\n",
    "        # Printing top keywords for each topic\n",
    "        if verbose == True:\n",
    "            print(i, ', '.join(keywords))\n",
    "    \n",
    "    return pd.DataFrame({'keywords':ls_keywords, 'frequency':ls_freqs, 'topic_id':topic_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lda_df = display_keywords(lda, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams - Adding context by creating N-grams\n",
    "\n",
    "Obviously, reducing a document to a bag of words means losing much of its meaning - we put words in certain orders, and group words together in phrases and sentences, precisely to give them more meaning. If you follow the processing steps we've gone through so far, splitting your document into individual words and then removing stopwords, you'll completely lose all phrases like \"kick the bucket,\" \"commander in chief,\" or \"sleeps with the fishes.\" \n",
    "\n",
    "One way to address this is to break down each document similarly, but rather than treating each word as an individual unit, treat each group of 2 words, or 3 words, or *n* words, as a unit. We call this a \"bag of *n*-grams,\" where *n* is the number of words in each chunk. Then you can analyze which groups of words commonly occur together (in a fixed order). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer= \"word\", # unit of features are single words rather then phrases of words \n",
    "                            tokenizer=tokenize, # function to create tokens\n",
    "                            ngram_range=(0,2), # Allow for bigrams\n",
    "                            strip_accents='unicode',\n",
    "                            stop_words=eng_stopwords,\n",
    "                            min_df = 0.001,\n",
    "                            max_df = 0.999)\n",
    "\n",
    "# Creating bag of words\n",
    "bag_of_words = vectorizer.fit_transform(abstracts) #transform our corpus is a bag of words \n",
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Fitting LDA model\n",
    "lda = LatentDirichletAllocation(n_components = 10, learning_method='online') \n",
    "doctopic = lda.fit_transform( bag_of_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the top keywords in each topic\n",
    "lda_df = display_keywords(lda, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color ='red'>**Question 1: Take the code above and run it again but let it include tokens that are three words long as well. Are there any three word sequences that show up in the top words?**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF - Weighting terms based on frequency\n",
    "\n",
    "One additional step we can add in cleaning and processing our text data is **Term Frequency-Inverse Document Frequency (TF-IDF)**. TF-IDF is based on the idea that the words (or terms) that are most related to a certain topic will occur frequently in documents on that topic, and infrequently in unrelated documents.  TF-IDF re-weights words so that we emphasize words that are unique to a document and suppress words that are common throughout the corpus by inversely weighting terms based on their frequency within the document and across the corpus.\n",
    "\n",
    "Recall that our data might look something like this:\n",
    "\n",
    "|document ID|about|america|author|ask|...|\n",
    "|-|-|-|-|-|-|\n",
    "|1|0|0|0|0|...|\n",
    "|2|0|1|0|0|...|\n",
    "|3|0|0|3|0|...|\n",
    "|4|1|0|0|0|...|\n",
    "|5|0|0|0|2|...|\n",
    "|...|...|...|...|...|...|\n",
    "\n",
    "The values that are in the cells are the term frequencies. TF-IDF takes those values and re-weights them by the inverse of how often they occur in other documents. So, for example, if the term occurs in many other documents, the term frequency would be close to 1 (since the fraction of documents the term occurs in is close to 1). However, if the term occurs only in a smaller fraction of documents (such as 1/10th of documents), then the term frequency is multiplied by a much larger number (since we use the inverse document frequency).\n",
    "\n",
    "Let's look at how to use TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english') + ['said', 'new', 'year', 'one', 'case']\n",
    "full_stopwords = [tokenize(s)[0] for s in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer= \"word\", # unit of features are single words rather then phrases of words \n",
    "                            tokenizer=tokenize, # function to create tokens\n",
    "                            ngram_range=(0,2),\n",
    "                            strip_accents='unicode',\n",
    "                            stop_words=full_stopwords,\n",
    "                            min_df = 0.001,\n",
    "                            max_df = 0.999)\n",
    "# Creating bag of words\n",
    "bag_of_words = vectorizer.fit_transform(abstracts) #transform our corpus is a bag of words \n",
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Use TfidfTransformer to re-weight bag of words \n",
    "transformer = TfidfTransformer(norm = None, smooth_idf = True, sublinear_tf = True)\n",
    "tfidf = transformer.fit_transform(bag_of_words)\n",
    "\n",
    "# Fitting LDA model\n",
    "lda = LatentDirichletAllocation(n_components = 5, learning_method='online') \n",
    "doctopic = lda.fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color ='red'>**Question 2: Look at the top words after implementing TF-IDF. What are the differences compared to without using TF-IDF?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  Next Steps: Document Classification with Supervised Learning\n",
    "\n",
    "We used topic modeling to determine what topics were discussed within NYT articles. That is an example of unsupervised learning: we were looking to uncover structure in the form of topics, or groups of agencies, but we did not necessarily know the ground truth of what topics there were.\n",
    "\n",
    "We can also do supervised learning with text data. In supervised learning, we have a *known* outcome or label (*Y*) that we want to produce given some data (*X*), and in general, we want to be able to produce this *Y* when we *don't* know it, or when we *only* have *X*. \n",
    "\n",
    "In order to produce labels we need to first have examples our algorithm can learn from, a \"training set.\" In the context of text analysis, developing a training set can be very expensive, as it can require a large amount of human labor or linguistic expertise. **Document classification** is an example of supervised learning in which want to characterize our documents based on their contents (*X*). A common example of document classification is spam e-mail detection. Another example is *part-of-speech tagging* where *X* are individual words and *Y* is the part-of-speech. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
