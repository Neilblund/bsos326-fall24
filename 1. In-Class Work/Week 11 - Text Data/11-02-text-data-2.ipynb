{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Text Data\n",
    "\n",
    "Use **Code** cells to write and run any code you need to answer the question and **Markdown** cells to write out answers in words. After you are finished with the assignment, remember to download it as an **HTML file** and submit it in **ELMS**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "import re\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis over Time\n",
    "\n",
    "We've so far only looked at the New York Times API for one month. However, we have access to much more data than that. The NYT Archive API allows us to pull data for any given month, so we could have just pulled data for an entire year by looping over the months and using the API for each month. The code to do this is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nyt-key.txt', 'r') as f:\n",
    "    nyt_key = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "year = 2020\n",
    "month = 7\n",
    "\n",
    "base_url = f\"https://api.nytimes.com/svc/archive/v1/{year}/{month}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r = get(base_url, params= {'api-key':nyt_key})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_nyt_archive(month, year, key):\n",
    "    '''\n",
    "    Pull from NYT Archive API for a given month and year. Returns a DataFrame that contains the abstract \n",
    "    \n",
    "    Arguments:\n",
    "        month: int, month for which the data should be pulled\n",
    "        year: int, year for which the data should be pulled\n",
    "        key: str, the Census key to use to pull from the API\n",
    "        \n",
    "    Returns:\n",
    "        A DataFrame\n",
    "    '''\n",
    "    base_url = f\"https://api.nytimes.com/svc/archive/v1/{year}/{month}.json\"\n",
    "    r = get(base_url, params= {'api-key':nyt_key})\n",
    "    articles = r.json()['response']['docs']\n",
    "    \n",
    "    keys = ['web_url','abstract', 'pub_date', 'type_of_material','word_count']\n",
    "    nyt_dict = {key:[article[key] for article in articles] for key in keys}\n",
    "    return pd.DataFrame(nyt_dict)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "archive_dfs = []\n",
    "\n",
    "for month in range(1,13):\n",
    "    archive_dfs.append(get_nyt_archive(month, 2020, nyt_key))\n",
    "    time.sleep(12)\n",
    "    \n",
    "nyt_2020 = pd.concat(archive_dfs).reset_index(drop = True)\n",
    "nyt_2020['month'] = nyt_2020.pub_date.str[5:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is a `time.sleep(12)` in the loop. This is because the NYT API only allows 5 API requests per minute. So, we slow down our code so that we don't make too many requests, or else they will be denied. This code might take some time to run because of this, so instead you can bring in the CSV file provided that has the results of this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nyt_2020 = pd.read_csv('nyt_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nyt_2020.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing NYT Abstracts Over Time\n",
    "\n",
    "First, let's take the data over a full year and apply the same cleaning techinques that we discussed. That is, we want to tokenize, stem, and remove stopwords within our corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we tokenize the words. Using `reset_index()` afterwards means that we have an `index` variable that acts as our document ID variable. In other words, the `index` variable represents a unique abstract, while the `abstract` variable contains the words within those abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = nyt_2020.abstract.apply(tokenizer.tokenize).explode().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll merge this back in with our `type_of_material` and `month` data so that we have that to use with our visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "token_df = tokens.merge(nyt_2020[['type_of_material', 'month']], \n",
    "                        how = 'left', left_on = 'index', right_index = True).dropna()\n",
    "\n",
    "token_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we proceed with our data pre-processing steps. We first lowercase all words, then remove stopwords. Lastly, we stem the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "token_df['abstract'] = token_df['abstract'].str.lower()\n",
    "cleaned_token_df = token_df[-token_df.abstract.isin(eng_stopwords)]\n",
    "cleaned_token_df['abstract'] = cleaned_token_df.abstract.apply(stemmer.stem)\n",
    "cleaned_token_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to look at the most frequent words within each month of the year. We could use `groupby` to agggregate by the `month` variable, then use `value_counts()` in order to count up how often each word was used within that month. Then, we'll just take the top 10 words from each month by using the `nlargest` method within each group. The `reset_index()` here again makes it so that we can pull out the grouped multi-index month and abstract as variables within the DataFrame, so that we can use them within our visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens_freq = cleaned_token_df.groupby('month').abstract.value_counts()\n",
    "top_tokens = tokens_freq.groupby(level = 0, group_keys=False).nlargest(10).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possibility for a graph is a bar graph with a slider for month. This allows us to look at the top words within each month separately, as well as see how it changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "px.bar(top_tokens, x = 'count', y = 'abstract', orientation = 'h',\n",
    "       animation_frame=\"month\", animation_group = 'abstract',\n",
    "       range_x=[0,1200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also try to use a line plot. This might not be as helpful with every word, because there are too many colors to tell apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.relplot(top_tokens, x = 'month', y = 'count', hue = 'abstract', kind = 'line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color ='red'>**Question 1: What are some trends in the words used each month of 2020? What are some possible relationships that might be of interest in this dataset, and what are some other possible types of graphs you might use to analyze these?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning Text Data into a Matrix Format\n",
    "\n",
    "When we work with data, we usually think about it in terms of rows and columns. That is, we have rows of observations and columns of variables. Text data as it is doesn't quite fit into that format, so we need to do some work to be able to do more advanced analyses. We've gone over doing some cleaning and breaking down of text data, but now we want to convert it into a matrix or table format that we can use to do analyses.\n",
    "\n",
    "To do this, we're going to treat each token as a variable and each document as an observation. So, in the case of NYT Article abstracts, we will treat individual article abstract as an observation. There will be as many columns as there are unique tokens in the overall corpus (so there will be many many variables!). The dataset that we end up with will looking something like this:\n",
    "\n",
    "|document ID|about|america|author|ask|...|\n",
    "|-|-|-|-|-|-|\n",
    "|1|0|0|0|0|...|\n",
    "|2|0|1|0|0|...|\n",
    "|3|0|0|3|0|...|\n",
    "|4|1|0|0|0|...|\n",
    "|5|0|0|0|2|...|\n",
    "|...|...|...|...|...|...|\n",
    "\n",
    "To convert our abstracts into this format, we first take a Series of the abstracts with everything lowercased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "abstracts = nyt_2020.abstract.str.lower().reset_index().abstract.dropna()\n",
    "abstracts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a `tokenize` function that does the tokenizing and temming steps that we had done before. This is a function that we will need to provide to `CountVectorizer` below instead of using directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return [stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## CountVectorizer\n",
    "\n",
    "We can apply this to each abstract in our corpus using `CountVectorizer`. This will not only do the tokenizing, but it will also count any duplicates of words and create a matrix that contains the frequency of each word. This will be quite a large matrix (number of columns will be number of unique words), so it outputs the data as a sparse matrix.\n",
    "\n",
    "We will first create the `vectorizer` object (you can think of this like a model object), and then fit it with our abstracts. This should give us back our overall corpus bag of words, as well as a list of features (that is, the unique words in all the abstracts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize stop words to match\n",
    "eng_stopwords = [tokenize(s)[0] for s in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer= \"word\", # unit of features are single words rather then phrases of words \n",
    "                             tokenizer=tokenize, # function to create tokens\n",
    "                             ngram_range=(0,1), # Tokens are individual words for now\n",
    "                             strip_accents='unicode',\n",
    "                             stop_words= eng_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have created the vectorizer, we can use it to transform our abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bag_of_words = vectorizer.fit_transform(abstracts) #transform our corpus into a bag of words \n",
    "features = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since this can be quite large, it will be stored as a sparse matrix. That is, it only stores information about which rows and columns have non-zero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bag_of_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features[[10649, 22204, 25233, 18085, 26651, 9483, 5041, 5005]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "abstracts[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
