{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89c608b-27e4-427c-9a94-58678b384365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "import re\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer, LancasterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f4ff77-b0da-48c7-9329-c21d5b0a9254",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_df = pd.read_csv('genius_lyrics.csv')\n",
    "lyrics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456db4aa-1b27-46c9-91cd-02b8c24aa69d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Processing Text Data\n",
    "\n",
    "**Text analysis** is used to extract useful information from or summarize a large amount of unstructured text stored in documents. This opens up the opportunity of using text data alongside more conventional data sources (e.g. surveys and administrative data). The goal of text analysis is to take a large corpus of complex and unstructured text data and extract important and meaningful messages in a comprehensible way. \n",
    "\n",
    "Text analysis can help with a wide variety of tasks including:\n",
    "\n",
    "* **Information Retrieval**: Find relevant information in a large database, such as a systematic literature review, that would be very time-consuming for humans to do manually. \n",
    "\n",
    "* **Clustering and Text Categorization**: Summarize a large corpus of text by finding the most important phrases, using methods like topic modeling. \n",
    "\n",
    "* **Text Summarization**: Create category-sensitive text summaries of a large corpus of text. \n",
    "\n",
    "* **Machine Translation**: Translate documents from one languagem to another. \n",
    " \n",
    "## Glossary of Terms\n",
    "\n",
    "* **Corpus**: A corpus is the set of all text documents used in your analysis; for example, your corpus of text may include hundreds of abstracts from patent data.\n",
    "\n",
    "* **Tokenize**: Tokenization is the process by which text is separated into meaningful terms or phrases. In English this is easy to do for individual words, as they are separated by whitespace; however, it can get more complicated to  automate determining which groups of words constitute meaningful phrases. \n",
    "\n",
    "* **Stemming**: Stemming is normalizing text by reducing all forms or conjugations of a word to the word's most basic form. In English, this can mean making a rule of removing the suffixes \"ed\" or \"ing\" from the end of all words, but it gets more complex. For example, \"to go\" is irregular, so you need to tell the algorithm that \"went\" and \"goes\" stem from a common lemma, and should be considered alternate forms of the word \"go.\"\n",
    "\n",
    "* **Stop Words**: Stop words are words that have little semantic meaning but occur very frequently, like prepositions, articles and common nouns. For example, every document (in English) will probably contain the words \"and\" and \"the\" many times. You will often remove them as part of preprocessing using a list of stop words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48a6e0d-0a2f-42c0-9dc6-40f0285b04c6",
   "metadata": {},
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4282f9-9af8-417b-949c-b6c25b169a8a",
   "metadata": {},
   "source": [
    "First, we will lowercase every word in the abstract column so that we don't run into issues with matching words that aren't capitalized in the same way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc2acf1-fed2-4bd6-a44c-c9b6e4f014d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lyrics_df['lyrics_processed'] =  lyrics_df.lyrics.str.lower()\n",
    "lyrics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d86da7d-dbee-44d8-aa2e-9becdcc94b7f",
   "metadata": {},
   "source": [
    "## Removing Punctuation and Tokenizing\n",
    "\n",
    "For some purposes, we might want to preserve punctuation. For example, if we wanted to be able to detect sentiment of text, we might want to keep exclamation points, because they signify something about the text. For our purposes, however, we will simply strip the punctuation so that it does not affect our analysis. \n",
    "\n",
    "We also want to separate text into individual tokens (generally individual words). To do this all of this, we'll use `RegexpTokenizer` to break apart individual words and identify tokens according to a regular expression. Here, we use the regular expression `\\w+`, which catches all \"words\" which contain letters or numbers. Anything that is not a number, such as punctuation or spaces, will not be included and be considered a separator between tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25421c89-77fd-43e0-8044-20baaa0145c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762c7e1e-6990-4fe6-ad43-3eefb3c75874",
   "metadata": {},
   "source": [
    "Here's an example of tokenizing the lyrics for the first song and printing the first 50 words. Notice that contractions (\"I'm\") are split into separate terms here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1afc24-0d0c-4ee0-84aa-c98dd5095c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.tokenize(lyrics_df.lyrics_processed[0])[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f03139-2cdd-47e0-9685-8572f4cf31f6",
   "metadata": {},
   "source": [
    "<font color ='red'>**Question 1: Apply the tokenizer to each  in `lyrics_df`. You should get a pandas Series of lists, with each list representing the tokenized abstract. Call this Series `bag_of_words`.**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ac5729-9c36-4acf-92fd-9700c25837ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc30e770-5207-49b1-8206-1a18d0aaa00d",
   "metadata": {},
   "source": [
    "Now we'll put the processed texts in a data frame and use \"explode\" to unnest each series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2982b51d-b904-44f0-84ae-dd3506697230",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_and_data = pd.DataFrame({'lyrics_processed': bag_of_words, \n",
    "                               'genre': lyrics_df.genre,\n",
    "                               'artist' : lyrics_df.artists,\n",
    "                               'title' : lyrics_df.titles,\n",
    "                               'url' : lyrics_df.url\n",
    "                              }).explode('lyrics_processed')\n",
    "\n",
    "lyrics_and_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef342063-8fd4-48af-a39a-35e8813ed441",
   "metadata": {},
   "source": [
    "### Visualizing the Data\n",
    "\n",
    "Let's take a look at our data to see what types of words are most common. This is a common intermediate step that is used to simply get a sense for what is in our dataset. Remember, we are converting blocks of text into individual words to try to get a sense for what these songs are about. (you might notice that our results here are not especially informative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8772404-4394-45a8-814b-527e26127f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ten_words = lyrics_and_data.lyrics_processed.value_counts().head(15)\n",
    "px.bar(top_ten_words[::-1], orientation = 'h',\n",
    "       labels = {'lyrics_processed':\"lyrics\",\n",
    "                 'value': 'count'\n",
    "               }\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7c43f9-8f9c-477b-afe3-4ee346496ccb",
   "metadata": {},
   "source": [
    "## Removing meaningless text - Stopwords\n",
    "\n",
    "Stopwords are words that are found commonly throughout a text and carry little semantic meaning. Examples of common stopwords are prepositions (\"to\", \"on\", \"in\"), articles (\"the\", \"an\", \"a\"), conjunctions (\"and\", \"or\", \"but\") and common nouns. For example, the words *the* and *of* are totally ubiquitous, so they won't serve as meaningful features, whether to distinguish documents from each other or to tell what a given document is about. You may also run into words that you want to remove based on where you obtained your corpus of text or what it's about. There are many lists of common stopwords available for you to use, both for general documents and for specific contexts, so you don't have to start from scratch.   \n",
    "\n",
    "We can eliminate stopwords by checking all the words in our corpus against a list of commonly occuring stopwords that comes with NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b559ec3-8565-4d87-8ac5-eb7b3da04e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05071bf9-f1ce-476b-a5d0-167b5f631a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "stop[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f2f15-ffb6-4821-91d4-dc6c026f95d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_and_data['stopword'] = lyrics_and_data.lyrics_processed.isin(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ad5ddb-459e-4a92-864a-06483681c7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing the result\n",
    "lyrics_and_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76adaa0d-add0-4ea3-a662-b59f51ec65e2",
   "metadata": {},
   "source": [
    "<font color ='red'>**Question 2: Make a plotly bar chart showing the top 15 terms excluding stopwords**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0b5918-dc0f-4785-ba02-f956c9853a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05cba03a-0d18-46ec-a674-edfa0fd73682",
   "metadata": {},
   "source": [
    "We'll go ahead and filter out the stopwords entirely for the next part of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810e3952-ba53-4a5f-b657-bc9747eea605",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_filtered = lyrics_and_data[lyrics_and_data.stopword==False].drop(columns='stopword')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929446eb-5ed0-4b7e-ab81-1aea69a8f0ac",
   "metadata": {},
   "source": [
    "## Stemming or Lemmatization - Distilling text data\n",
    "\n",
    "We'll usually try to simplify our bag-of-words analysis by grouping together different inflections of the same terms. Word-endings that indicate things like pluralization and tense are useful in the context of human communication, but they're not informative when we're trying to do things like identify the topic or tone of a text.\n",
    "\n",
    "There are two common approaches to this kind of normalization: \n",
    "\n",
    "- **Lemmatization** uses parts-of-speech and context clues to convert words to their basic dictionary form. \n",
    "- **Stemming** uses some simple hueristics to remove word inflections. \n",
    "\n",
    "Stemming is more error-prone than lemmatization, and sometimes results in words that you won't find in a dictionary, but it has the advantage of being much faster because it relies in simple rules whereas lemmatization considers word context and parts-of-speech. \n",
    "\n",
    "There are multiple stemming algorithms with different rule sets and differing strengths and weaknesses. In this notebook, we'll use the Snowball Stemmer. You'll notice this works pretty well for many words, but gives odd results for others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef421224-bfd4-4105-b96d-4045e4fd1c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the stemming algorthim\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a437bca9-5ee4-489a-b783-73b446827e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply it to some terms\n",
    "forms = ['lying', 'fisherman', 'change', 'systematic', 'stapled', 'catlike', 'argument', 'alphabetical']\n",
    "print([stemmer.stem(i) for i in forms])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d345a733-54bd-440f-bffe-7841bc5db5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the snowball stemmer to the lyrics data\n",
    "lyrics_filtered['lyrics_stemmed'] = lyrics_filtered.lyrics_processed.apply(stemmer.stem)\n",
    "lyrics_filtered.head(n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a1a0e9-12bf-4fff-95fb-d26898866d95",
   "metadata": {},
   "source": [
    "<font color ='red'>**Question 3: Try stemming using an alternative stemmer: the Lancaster stemmer. How does it differ from the Snowball Stemmer?**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b59ea5-0f23-4af8-8a34-44b227ad05d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lancaster = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb6291e-db37-441a-9429-820c37a5780f",
   "metadata": {},
   "source": [
    "# Terms by genre\n",
    "\n",
    "\n",
    "Now, we'll try calculating the top words separately for each genre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd6baf3-7018-4857-a4d0-d35c654f04fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_words =lyrics_filtered.value_counts('lyrics_stemmed').head(10)\n",
    "terms_by_genre = pd.crosstab(lyrics_filtered.lyrics_stemmed, lyrics_filtered.genre, normalize='columns')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b474b532-3e2f-45a2-ba90-80eaeaa439b8",
   "metadata": {},
   "source": [
    "Let's keep only the most frequent words so that we don't have too many to look at at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a13512-695d-4294-9407-516567290afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_by_genre = terms_by_genre.loc[top_words.index[::-1],:]\n",
    "top_words_by_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d882f7-a086-45d7-948f-bfa2b052ee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# could also do top terms separately for each genre with this slightly messy code: \n",
    "#terms_by_genre_long = terms_by_genre.melt(ignore_index=False).sort_values(by='value', ascending=False).groupby('genre').head(n=5)\n",
    "#terms_by_genre_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b262eadf-950c-483e-a4c5-33374b0a33ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.bar(top_words_by_genre, \n",
    "             y = top_words_by_genre.columns, \n",
    "             x = top_words_by_genre.index, \n",
    "             barmode='group',\n",
    "       orientation = 'v', \n",
    "            labels = {'lyrics_stemmed': 'lyric',\n",
    "                      'value': 'proportion of all terms'\n",
    "                     }\n",
    "            )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce06a06-6e15-4ae5-b1a4-8d45932a8f56",
   "metadata": {},
   "source": [
    "<font color ='red'>**Question 4: The code below generates a table with the number of unique stemmed terms per song. Make a plot showing the distribution of the variable across each genre**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254bcd19-bd96-4a93-88f4-b45392ba739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_counts = lyrics_filtered.groupby(['title', 'genre', 'artist'])['lyrics_stemmed'].nunique().reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4795110d-925f-4624-9531-8d041c7fe470",
   "metadata": {},
   "source": [
    "# Extra stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c807f9e-c0a3-4c97-8c06-bf99563795eb",
   "metadata": {},
   "source": [
    "## Finding Key terms\n",
    "\n",
    "What terms are disproportionately likely to occur in a country music compared to other genres? There are a number of metrics used to calculate this stat, called \"keyness\". The code below uses the log base 2 ratio of proportions between two corpuses. Positive numbers indicate that a term is more associated with the \"target\" group, and negative numbers indicate that they're more strongly associated with the \"negative\" group.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdb53fd-9ded-46fc-b26e-8ddd5473a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "target = lyrics_filtered.loc[lyrics_filtered['genre'] == 'country'].value_counts('lyrics_stemmed')\n",
    "baseline = lyrics_filtered.loc[lyrics_filtered['genre'] !='country'].value_counts('lyrics_stemmed')\n",
    "\n",
    "keyness = []\n",
    "target_total = sum(target)\n",
    "baseline_total = sum(baseline)\n",
    "norm = 1/ (baseline_total + target_total)\n",
    "combined =  target.index.union(baseline.index)\n",
    "# placing a minimum threshold on the total number of word occurrences\n",
    "minimum = 10\n",
    "\n",
    "for i in combined:\n",
    "    # adds a small value for terms that don't occur at all\n",
    "    target_count = target[i] if i in target.index else norm\n",
    "    baseline_count = baseline[i] if i in baseline.index else norm\n",
    "    if (baseline_count + target_count) < minimum: \n",
    "        continue\n",
    "    target_prop = ((target_count/target_total)) \n",
    "    baseline_prop = ((baseline_count/baseline_total)) \n",
    "    stats = {'term': i,\n",
    "             'count_target' : int(target_count),\n",
    "             'count_baseline': int(baseline_count),\n",
    "             'oddsratio': math.log2( target_prop/baseline_prop)}\n",
    "    keyness.append(stats)\n",
    "        \n",
    "\n",
    "# put the results in a data frame and sort them\n",
    "df=pd.DataFrame(keyness).sort_values('oddsratio', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834c2e7f-8eb9-4e50-a5a3-a909dac08565",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de61a07-e866-4141-a168-133814aead12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping it in a function definition\n",
    "\n",
    "def keyness(target, baseline, minimum=10):\n",
    "    \"\"\"\n",
    "    Given two sets of value counts, calculate keyness using the log 2 ratio of\n",
    "    term proportions. \n",
    "    \"\"\"\n",
    "    keyness = []\n",
    "    target_total = sum(target)\n",
    "    baseline_total = sum(baseline)\n",
    "    norm = 1/ (baseline_total + target_total)\n",
    "    combined =  target.index.union(baseline.index)\n",
    "    for i in combined:\n",
    "        # adds a small value if the term doesn't occur anywhere in the corpus\n",
    "        target_count = target[i] if i in target.index else norm\n",
    "        baseline_count = baseline[i] if i in baseline.index else norm\n",
    "        if (baseline_count + target_count) < minimum: \n",
    "            continue\n",
    "        target_prop = (target_count/target_total)\n",
    "        baseline_prop = (baseline_count/baseline_total)\n",
    "        stats = {'term': i,\n",
    "                 'count_target' : int(target_count),\n",
    "                 'count_baseline': int(baseline_count),\n",
    "                 'oddsratio': math.log2( target_prop/baseline_prop)}\n",
    "        keyness.append(stats)\n",
    "        \n",
    "\n",
    "        # put the results in a data frame and sort them\n",
    "    keydata=pd.DataFrame(keyness).sort_values('oddsratio', ascending=False)\n",
    "    return(keydata)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a27ca66-2e13-4d52-8891-e248928b46e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what terms are most strongly associated with Drake? \n",
    "\n",
    "df=keyness(lyrics_filtered.loc[lyrics_filtered['artist'] == 'Drake'].value_counts('lyrics_stemmed'), \n",
    "           lyrics_filtered.loc[lyrics_filtered['artist']!='Drake'].value_counts('lyrics_stemmed'),\n",
    "           minimum= 5)\n",
    "\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8699be-0044-45bd-b7ed-ef63a83d4842",
   "metadata": {},
   "source": [
    "# Example of lemmatization\n",
    "\n",
    "This may be overkill for what we're doing here, but here's an example of using lemmatization instead of stemming for the song Old Town Road. You'll notice that the lemmatization is able to do things like convert \"to be\" verbs to their base form and separate contractions like \"can't\" into separate words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1027752-be02-431e-85a7-0244c2097e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = lyrics_df.loc[lyrics_df['titles'] == \"Old Town Road\", 'lyrics'].item()\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fb5a24-fed2-4da5-a54e-2427e9362a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3541dc-32a1-4490-93eb-6c29d7094da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    " \n",
    "# Process the text using spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# originals\n",
    "lowercase_tokens = [token.lower_ for token in doc]\n",
    "\n",
    "# stemmed\n",
    "stemmed_tokens = [stemmer.stem(token.lower_) for token in doc]\n",
    "\n",
    "# lemmatized\n",
    "lemmatized_tokens = [token.lemma_ for token in doc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b4283b-0cde-4a98-ac98-cd93df677677",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# putting it all in a data frame for comparison: \n",
    "\n",
    "pd.DataFrame({\"original\": lowercase_tokens, \n",
    "             \"stemmed\": stemmed_tokens,\n",
    "              \"lemma\": lemmatized_tokens\n",
    "             }).head(n = 25)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
