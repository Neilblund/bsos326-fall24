{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de637ffc-3ed2-414b-b3b1-1622332bca10",
   "metadata": {},
   "source": [
    "# Gathering data in loops\n",
    "\n",
    "Your final project will probably require you to send more than one API request or scrape more than one page. The next sections offer some general tips on how you can write code that can collect large amounts of data from an API. The final section gives a few tips on organizing your projects. \n",
    "\n",
    "Regardless of how you choose to organize it, your final project must have code that can be used to replicate or update the results you use in your analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d484ac7f-c58f-4b34-be19-f9bb64e39fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12205435-56db-4604-8c20-4c3753145117",
   "metadata": {},
   "source": [
    "## API Pagination\n",
    "\n",
    "Many APIs will place a limit on the number of results you can retrieve with a single query. In order to collect a complete data set, you'll usually need to write a loop that sends multiple requests until you've collected all of the relevant data.\n",
    "\n",
    "The exact process for doing this will vary depending on the API, but usually it will involve using either an offset or a pagination parameter.\n",
    "\n",
    "We can use the example from the World Bank Development Indicators API to illustrate how to do this. This query returns carbon emissions for all countries in 2020:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e7a262-32ee-4d1f-aa62-43dd05611f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdi_params = {'format':'json',\n",
    "              'per_page':100,\n",
    "              'date':2020\n",
    "             }\n",
    "url = 'https://api.worldbank.org/v2/country/all/indicator/EN.ATM.CO2E.PC'\n",
    "response = get(url, params = wdi_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268122cc-9890-4f72-9f21-76ae6f6f1b18",
   "metadata": {},
   "source": [
    "This query only returns the first 100 results, but the response object tells us how many more results are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8720c91d-1722-4e00-beb0-b9103982cb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc20be65-72e4-410d-93b1-82e0e1402fdf",
   "metadata": {},
   "source": [
    "[According to the documentation for this API](https://datahelpdesk.worldbank.org/knowledgebase/articles/898581), we can get the next page of results by incrementing the `page` parameter in our request. So the next page of results would just add \"&page=2\" to the URL we just requested. \n",
    "\n",
    "We could just write all three links out separately, but a more generalizable approach would be to write a loop that makes use of the pagination information that the API gives us. The code below uses a `while` loop to continuously send requests until we reach the final page. After running it, we'll have a list of responses that we can then concatenate into a single data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bc45d7-3da6-4bae-9be0-1ebce4307772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with an empty list\n",
    "results_list = []\n",
    "\n",
    "morepages = True\n",
    "i = 1\n",
    "\n",
    "while morepages == True:\n",
    "    wdi_params = {'format':'json',\n",
    "              'per_page':100,\n",
    "              'date':2020, \n",
    "               'page':i}\n",
    "    url = 'https://api.worldbank.org/v2/country/all/indicator/EN.ATM.CO2E.PC'\n",
    "    response = get(url, params = wdi_params)\n",
    "    # append page i to results_list\n",
    "    results_list.append(response)\n",
    "    # check to see if we've reached the final page:\n",
    "    morepages = i < response.json()[0].get('pages')\n",
    "    \n",
    "    time.sleep(1)\n",
    "    i +=1\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff154fc-247a-4685-b55d-e0b06c6fae9c",
   "metadata": {},
   "source": [
    "Now we just need to format and concatenate all the results. To do that, I've written a function that takes a single response from the WDI API and turns it into a data frame. I'll apply it to each list element using a list comprehension, and then use `pd.concat` to create a single data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbcba22-bbb1-4e48-a8ce-b9d891e25064",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def wdi_parser(resp):\n",
    "    result_dict = [{'country_id':i['countryiso3code'],\n",
    "                    'country_name':i['country']['value'],\n",
    "                    'date': int(i['date']),\n",
    "                    'indicator': i['indicator']['id'],\n",
    "                    'indicator_description' : i['indicator']['value'],\n",
    "                    'indicator_value': np.float64(i['value'])} for i in resp.json()[1]]\n",
    "    return pd.DataFrame(result_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbe2647-9a2c-46a2-a176-dee9ed1f2723",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_responses = [wdi_parser(i) for i in results_list]\n",
    "wdi_df = pd.concat(parsed_responses)\n",
    "wdi_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3f9cae-0874-485e-9278-bb8952974057",
   "metadata": {},
   "source": [
    "Now, we should have results for all 266 countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dda600-caff-46e3-b975-4942bb05eaab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wdi_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798399b8-515a-44dc-bbb3-b6f863646afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.json()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763181b8-2ffc-4ec7-b685-8c0a9221e22c",
   "metadata": {},
   "source": [
    "### Pagination with offsets\n",
    "Keep in mind that the process of paginating through data will not always be the same across all APIs. For instance: the [Nobel Prize API](https://app.swaggerhub.com/apis/NobelMedia/NobelMasterData/2.1) uses an offset parameter rather than a pagination parameter. So you would write something like `offset=0&limit=100` to get results 1-100, and then you would increment that by 100 (`offset=100&limit=100`) to get 101 through 200 and so on and you would continue until your offset was greater than or equal to the maximum number of responses. \n",
    "\n",
    "However, while the specific parameters might be different, the basic ingredients for pagination are more-or-less the same:\n",
    "1. You need code that takes a response object and then creates a URL to retrieve the next page of data\n",
    "2. You need code that can detect when there are no pages left\n",
    "3. You need code to format all of the pages into a single data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72077e0-055a-4d42-a861-a912430685da",
   "metadata": {},
   "source": [
    "<b style=\"color:red;\">\n",
    "<h3>Question 1A</h3> The request below gets a single page of results from the PokeApi (see <a href ='https://pokeapi.co/docs/v2#pokemon'>documentation</a>) Start by writing code that will retrieve/create a request for the next page of data</b>\n",
    "\n",
    "(Note that you can either use an offset parameter or the \"next\" url to get results here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc48b85-085e-4e9b-ae1b-7ff220e9ac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'offset':0,\n",
    "         'limit':100\n",
    "         }\n",
    "request = get('https://pokeapi.co/api/v2/pokemon', params=params)\n",
    "\n",
    "request.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea085342-143f-4a74-b0c5-a2bce10b440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to get the next page of results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09db695-158f-45e5-9709-1eca5592af83",
   "metadata": {},
   "source": [
    "<b style=\"color:red;\">\n",
    "<h3>Question 1B</h3> The request below shows you what the final page of data would look like. Use this response to write some code that will return `False` if we've reached the final page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db57d87-8807-4b71-b3dd-6d1528aa6178",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'offset':request.json()['count']-10,\n",
    "         'limit':100\n",
    "         }\n",
    "request = get('https://pokeapi.co/api/v2/pokemon', params=params)\n",
    "request.url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941dfa26-2282-4e91-b855-1bc92ae203e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c6672f0-58ed-414f-af9b-2fe49757f9bd",
   "metadata": {},
   "source": [
    "<b style=\"color:red;\">\n",
    "<h3>Question 1C</h3>\n",
    "Use the code above to create a while loop that iterates through each page of results and collects the name and url of each Pokemon in a list. Remember to put a short pause between each iteration of the loop. </b>\n",
    "\n",
    "If you find your loop runs for a really long time, you might want to interrupt the kernal by pressing the stop button at the top of your notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7897437f-ec9b-420a-b20f-cc6b0d4c7c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to create a list with all the responses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab7f846-275c-4331-b35a-9801da3f74eb",
   "metadata": {},
   "source": [
    "<b style=\"color:red;\">\n",
    "<h3>Question 1D</h3>\n",
    "Take a single element from your list of responses and write a function that will turn it into a dataframe. Then apply that function to your list of results from the previous step using a list comprehension and use `pd.concat` to combine them all together\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8533cb77-fb6f-4cf9-951f-7706a6089013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to concatenate everything in a data frame \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b8c827-d822-46d9-87c3-75d6950ede36",
   "metadata": {},
   "source": [
    "Once we're reasonably confident that we know how to navigate the pagination process, we might want to write a pagination function that can take any query and return the entire list of results. You can see an example of doing that with the Congress.gov API in the `congress_api_functions.py` file which is discussed at the end of this document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0b529a-438e-4470-845a-b0e10380d2ec",
   "metadata": {},
   "source": [
    "### A note on gathering complex data\n",
    "\n",
    "Depending on how the data are structured, there may be cases where you need to query one part of the API to get a URL for a separate endpoint that has more detailed data about that subject. The PokeApi is a good example of this: we retrieved a list of names and URLs, but if we navigate to any one of those URLs we'll get even more detailed information about the selected Pokemon. So if we wanted to create a data set with detailed information on each Pokemon, we would need to iterate over all of these URLs and then format all of our results in data frame. The way that data are organized is really up to the person who maintains the data set, so you'll want to spend some time getting to know an API before you can really get a good sense of what you can do with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89517dfd-1d27-4a89-b9bf-68bcc76f7ed0",
   "metadata": {},
   "source": [
    "## Scraping Multiple Pages\n",
    "\n",
    "Large scale web scraping can sometimes require us to do a different kind of \"pagination\" in order to visit multiple links on a page and extract some text or data from each one. For instance: [this URL](https://lite.cnn.com/) has a list of top stories from CNN.com and hyperlinks to (a minimal HTML version of) each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20f0692-949b-41c4-b89e-6f31c79979fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "site = get('https://lite.cnn.com/')\n",
    "content = BeautifulSoup(site.content, 'html.parser')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba72984-3498-4493-ba5a-74545bddd33c",
   "metadata": {},
   "source": [
    "I can extract the list of links and headlines and place it in a data frame like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c803fe95-0312-4e13-9114-cde07ec91cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = content.select('ul a')\n",
    "\n",
    "fmted_result ={'links' : [urljoin(site.url, i.get('href')) for i in headlines],\n",
    "               'headline_text' : [i.get_text().strip() for i in headlines]}\n",
    "\n",
    "article_df = pd.DataFrame(fmted_result)\n",
    "article_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df399ff9-b36c-4acc-9f98-1430cb382e6c",
   "metadata": {},
   "source": [
    "Note: the `urljoin` function here just turns a relative url into an absolute url (see [here](https://developer.mozilla.org/en-US/docs/Learn/Common_questions/Web_mechanics/What_is_a_URL#absolute_urls_vs._relative_urls) for some discussion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee1736a-fc94-4eca-991f-7941bbcf76e3",
   "metadata": {},
   "source": [
    "On its own, this list of links isn't very useful. More often I'll want to get the actual article text as well. To do that, I would need to write a loop that visits each page extracts the full text. \n",
    "\n",
    "For the sake of this example, I'll just take the first five headlines here. Once we've got code that works, we can easily re-run this loop on the entire list of headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b268a019-54d3-45b8-bc8f-868bbaf577a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding an empty column to the data frame\n",
    "#article_df['article_text'] = np.NaN\n",
    "\n",
    "# taking the first 5 rows just for this example \n",
    "article_sample = article_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d713cdc7-8718-44d6-8b43-a04d0c78e0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_sample.loc[1, 'links']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20753575-c3cc-4bf4-98f3-9c3c371f2406",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = []\n",
    "for link in article_sample['links']:\n",
    "    # navigate to link i\n",
    "    resp = get(link)\n",
    "    pages.append(resp)\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ff7a8d-d256-409f-a379-614c7982801f",
   "metadata": {},
   "source": [
    "<b style=\"color:red;\">\n",
    "<h3>Question 2A</h3>\n",
    "Write a loop to get a response object from each page in the sample of articles. Make sure to put a `time.sleep` call in your loop to put a short rest between each request\n",
    "</b>\n",
    "\n",
    "Hint: since you already know exactly how many URLs you need to visit, you can use a `for` loop instead of a `while` loop here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d025c4-4131-4099-be03-501d9154969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a loop to get responses from each link\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6a2dd6-19d7-4d6a-bc9a-cd160ce20773",
   "metadata": {},
   "source": [
    "<b style=\"color:red;\">\n",
    "<h3>Question 2B</h3>\n",
    "Write a function that takes a single response and returns one long string of text from a CNN article. Call the function <code>cnn_text</code>\n",
    "</b>\n",
    "\n",
    "Note that you can concatenate a list of strings like this:\n",
    "\n",
    "`' '.join(string_list)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173cd15f-35ae-47b5-9a8e-680715b5385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try working with a single result to start, then wrap your code in a function that takes \"x\" as an argument:\n",
    "# x = pages[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a3f032-00b0-4081-ac10-9d769a595f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_text(resp):\n",
    "    content = BeautifulSoup(resp.content, 'html.parser')\n",
    "    text = '\\n'.join([i.get_text() for i in content.select('.paragraph--lite')])\n",
    "    \n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3ce5f7-fee7-45e3-a7e6-e758b6350ff0",
   "metadata": {},
   "source": [
    "Now that we have a working function, we can apply it to the entire list and then use the `insert` function to add it to our existing data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1e0dbc-8aff-4d4f-9695-383df60b1c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [cnn_text(i) for i in pages]\n",
    "article_sample.insert(0, \"article_text\", text, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484c3e1a-9c25-4013-b70e-d1b82d573797",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_sample.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fc964e-7d32-40b5-87f5-5227b35a6db7",
   "metadata": {},
   "source": [
    "When we're done, it might be a good idea to save a copy of `article_sample` so we can work with it later. This is especially true when we're running code that sends a lot of requests.\n",
    "\n",
    "We can use the `.to_csv` method to store a result as a csv file. After running this you should see a file called `cnn_articles` in your working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcbfab6-9598-4741-8701-236286ccd5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usually set index=False to avoid writing the index names as a new column\n",
    "article_sample.to_csv('cnn_articles.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f630225-14a9-46dd-aec7-3cef965ef5a1",
   "metadata": {},
   "source": [
    "If you want to restore this data, then you would just run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70e7a59-5f14-4f75-a10f-4b025f0d7ba3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arts = pd.read_csv(\"cnn_articles.csv\")\n",
    "arts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e56e6-5e26-4b27-a4bb-15547007ce74",
   "metadata": {},
   "source": [
    "# Managing larger projects\n",
    "<a id='managing'></a>\n",
    "\n",
    "Up to now, we've mostly run all of our analyses in a single notebook file. This is fine for quick analysis, but when we start to assemble larger projects, we'll often want to maintain multiple scripts or notebooks to help us organize and separate our code. Whether and how you choose to do this is a matter of personal judgement. However, whatever you choose to do, all the code needed to replicate your analysis should be saved somewhere along with comments or written instructions on how you did things. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d426da-79fe-4aff-8937-f7f3085686e0",
   "metadata": {},
   "source": [
    "### Separating your code and saving results\n",
    "\n",
    "If we're just sending a couple API requests or scraping a single page, it often makes sense to do this \"in memory\": we scrape a page or send a request to an API, store the results in a Python variable, and run our analyses. The results of our analyses will go away whenever we exit Python, but we can just re-run the request next time we open open Python. This is fine for small-scale analysis, but, when we have code that sends a lot of requests, we probably don't want to have to re-run that over and over again. In those cases we probably want to write a separate script that collects our data and then saves the results to a file. Then we can just re-load that file next time we open Python\n",
    "\n",
    "For instance, rev.com has transcriptions of speeches from the 2024 presidential campaign. I want to scrape the full text of each speech. There are about 200 separate pages with transcripts, and I put a short pause of 1 second between each request, so this takes over 3 minutes to run. I don't want to do this every time I open Python, so I wrote a separate script called `speeches_scraper.py` that does the data collection and stores the results in a .csv. Then I can reload that data by running  `pd.read_csv` at the top of my script:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3b5da6-716b-4f19-a048-82b1bd2159b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "speeches = pd.read_csv(\"extra_code/speeches.csv\")\n",
    "speeches.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48d3a86-dd6c-4122-8492-d4a9bdc4d47e",
   "metadata": {},
   "source": [
    "When/whether it makes sense to separate code this way is often a matter of personal judgement, but if it takes more than a few seconds to run and you don't need/want the data to update, then this is probably your best option"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb09608-f672-41d8-835e-445e77b789b8",
   "metadata": {},
   "source": [
    "If you've written a Python script in a separate file, you can call it directly from within a Jupyter notebook using the `%run` magic command.\n",
    "\n",
    "Here's an example of conditionally running our scraper script from within a jupyter notebook. This code runs if the \"speeches.csv\" file doesn't exist in the current working directory, otherwise, it skips this script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14b347a-d85a-4d46-a40d-09fed9ac2fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "if os.path.isfile('extra_code/speeches.csv') == False:\n",
    "    print(\"speeches file doesn't exist, creating\")\n",
    "    %run ./extra_code/pres_speeches.py\n",
    "else:\n",
    "    print(\"file already exists. Skipping\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0627f1e6-ef2c-4816-b858-e16dba5b80ef",
   "metadata": {},
   "source": [
    "### Creating a functions file\n",
    "\n",
    "Another scenario where we might want to keep separate files is when we have a lot of function definitions that get re-used throughout our analysis and we want to avoid cluttering a written report.\n",
    "\n",
    "For instance, I've written a couple of functions for working with the congress.gov API. The `member_parser` function takes a response from the members endpoint and re-formats it as a dataframe. The `congress_paginate` function takes an initial requests and then automatically paginates until it gets a complete list of results from the API. Instead of including those functions in this document, I've placed them in a separate file and then I make them available in this notebook by calling `from congress_api_functions import congress_paginate, member_parser`. \n",
    "\n",
    "Note: this code assumes you've got a Congress.gov API key. You can sign up for one here: https://gpo.congress.gov/sign-up/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b51e7b-ec79-41a5-941a-7a0aab961e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import my custom functions: \n",
    "from extra_code.congress_api_functions import congress_paginate, member_parser\n",
    "\n",
    "# import the congress API key\n",
    "with open('extra_code/congress_gov.txt', 'r') as f:\n",
    "    congress_key = f.readline()\n",
    "\n",
    "# the members endpoint\n",
    "member_url = 'https://api.congress.gov/v3/member'\n",
    "# my additional parameters: \n",
    "congress_parameters = {'currentMember': 'true',\n",
    "                       'page':'1',\n",
    "                       'limit': 250,\n",
    "                       'api_key':congress_key}\n",
    "# running the pagination function:\n",
    "responses_list = congress_paginate(member_url, params= congress_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfc4c7b-a602-4c93-848d-b55e004576d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the list of responses, parse each one, and then create a single concatenated data frame: \n",
    "member_frame = pd.concat([member_parser(i) for i in responses_list])\n",
    "# look at the first few results: \n",
    "member_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6807a92-014a-4d5b-bd1d-d1084a1b59e3",
   "metadata": {},
   "source": [
    "As with the previous example, the decision to organize code this way is largely a matter of personal judgement. The main disadvantage of this approach is that it can make it harder for people to understand what your code is doing. On the other hand, it can make our notebook file more concise. \n",
    "\n",
    "Another advantage is that, if I'm writing multiple analyses that all require this set of functions, I can maintain a single functions file and the import a copy to each of my analyses. This helps me avoid writing redundant code, and it also makes it a lot easier to make modifications or correct errors in my functions since I only need to edit a single file instead of three.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238bb522-e5e4-4d77-b86b-8f3d906a48d4",
   "metadata": {},
   "source": [
    "[![](https://mermaid.ink/img/pako:eNp9zrEKwjAQBuBXCTcptIO6ZRDUrk51Mw5HcmkDTVLSC1JK390IOupN9x_fD7eAjoZAgh3iU_eYWNwaFUSZZnOdhc1Bs4th2oq6PorTvdXJjSx2j9_o_EX7P-jyRYeCoAJPyaMz5ZPlXVLAPXlSIMtqyGIeWIEKa6GYObZz0CA5Zaogxdz1IC0OU0l5NMjUOOwS-s91fQHh7kqx?type=png)](https://mermaid.live/edit#pako:eNp9zrEKwjAQBuBXCTcptIO6ZRDUrk51Mw5HcmkDTVLSC1JK390IOupN9x_fD7eAjoZAgh3iU_eYWNwaFUSZZnOdhc1Bs4th2oq6PorTvdXJjSx2j9_o_EX7P-jyRYeCoAJPyaMz5ZPlXVLAPXlSIMtqyGIeWIEKa6GYObZz0CA5Zaogxdz1IC0OU0l5NMjUOOwS-s91fQHh7kqx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eb1ed7-2ed4-4250-8f7d-44bd6885c57f",
   "metadata": {},
   "source": [
    "## Installing Additional Packages\n",
    "\n",
    "We've only used packages that were already available and installed in our BSOS JuypterHub environment so far. However, you might want to use packages that aren't already installed (or you might want to have JupyterLab on your own computers and need to install them yourself). To do this within Jupyter, you can use the ! notation and use the pip installer to install any packages. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09925061-26aa-4248-9790-777b49af2725",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079695c0-8840-4d7a-8dc8-fad19cf1f63a",
   "metadata": {},
   "source": [
    "The package above installs an Python interface for a `sqlite`, a lightweight SQL data base engine. While SQL is somewhat outside the scope of this class, its a useful tool to have in our tool kit if we want to be able to create and interact with very large databases because it allows us to work with datasets that are too large to hold in memory. \n",
    "\n",
    "If you're interested in trying out the sqlite3 package, there's a script in the extra code directory called `scraper_db.py` that gives a toy example of a script that creates/updates an SQL database with the transcripts from rev.com.\n",
    "\n",
    "Unlike the `speeches_scraper.py` code, this code checks scraped links against the links that are already in the database, and only scrapes them if they're new. Code like this can be used to efficiently update a database on at regular intervals (although in a really large dataset, you would probably want to handle this with an \"upsert\" operation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7dc4ed-2746-45ad-827e-9c9a6af5ca8b",
   "metadata": {},
   "source": [
    "If you're interested in seeing what a really well-designed project of this sort might look like, you can check out the open-source `count-love` crawler on github: https://github.com/count-love/crawler"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
