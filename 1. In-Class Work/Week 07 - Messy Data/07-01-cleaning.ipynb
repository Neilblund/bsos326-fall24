{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dating cleaning\n",
    "\n",
    "\n",
    "Use **Code** cells to write and run any code you need to answer the question and **Markdown** cells to write out answers in words. After you are finished with the assignment, remember to download it as an **HTML file** and submit it in **ELMS**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "import re\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sections will look at some strategies for cleaning real world data. We'll use some selected variables from the 2020 round of the <a href=\"https://electionstudies.org/data-center/2020-time-series-study/\">American National Election Study</a> which surveys eligible voters before and after each presidential election. \n",
    "\n",
    "This subset of the data contains the following variables: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable       | Description                                                                  |\n",
    "|----------------|------------------------------------------------------------------------------|\n",
    "| partyid        | R's preferred party (7 category)                                             |\n",
    "| id             | R's unique identifier                                                        |\n",
    "| libcon         | R's liberal/conservative self placement (7 category)                         |\n",
    "| biden_likes    | Open ended: what does R like about Joe Biden?                                |\n",
    "| biden_dislikes | Open ended: what does R dislike about Joe Biden?                             |\n",
    "| trump_likes    | Open ended: what  does R like about Donald Trump?                            |\n",
    "| trump_dislikes | Open ended: what does R dislike about Donald Trump?                          |\n",
    "| mipp_1         | Open ended: what is the most important political problem facing our country? |\n",
    "| age_group      | Age (5 categories)                                                           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_file = \"https://raw.githubusercontent.com/Neilblund/APAN/refs/heads/main/anes_selected.csv\"\n",
    "df = pd.read_csv(data_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing you might notice here is that many of the responses have `NaN`, which indicates missing or invalid responses. This is pretty common with survey data: some questions aren't asked because they're not relevant to all respondents, or respondents might hang up the phone, or just refuse to give a valid answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a sense of the number of `NaN` values using the `.info` method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, using `isnull()` gives us a data frame of the same size with `True` and `False` values depending on whether it was a missing value or not. Then, `mean()` calculates the arithmetic mean for each column. Since Python treats `True` as `1` and `False` as `0`, the mean of all of these values is the same as the proportion of missing data for each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color ='red'>**Question 1: the \"like/dislikes\" questions were left blank if a respondent said they couldn't identify anything they liked about a candidate. How would you show the relationship between party ID and being able to say anything positive about Trump?**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that there were no `NaN` values reported for the `partyid` and `libcon` variables, but, in reality, there are still non-responses for these questions, they're just given text labels instead of being left blank. We can see this by using the `value_counts` method. \n",
    "\n",
    "(note: we can use the `sort_index` method to sort these alphabetically instead of in order of the most frequent category. For ordinal variables like this one that have a clear ordering, that may often be a better option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.partyid.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.libcon.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, we will want to convert this kind of labelled missing data to an explicit `NaN` instead. This will prevent these responses from showing up in our tables and graphs. We can replace a value with an `NaN` like the `replace` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_values = [\"-9. Refused\", \"99. Haven't thought much about this\", \"-8. Don't know\"]\n",
    "\n",
    "df['libcon_replaced'] = df['libcon'].replace(missing_values, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.libcon.value_counts().sort_index(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color ='red'>**Question 2: Replace the refused/don't know values in the party ID variable with `NaN`**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when you're done, re-run the function to count nulls to make sure your code worked:\n",
    "df.isnull().mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recoding data\n",
    "\n",
    "In some cases it can make sense to collapse or rearrange one or more categories to simplify our data or capture a particular subset of respondents.  For instance, there's generally not a significant difference in the voting behavior of people who report that they are \"weak\" or \"independent\" partisans compared to people who self-described \"strong\" partisans, so for the sake of simplify we often collapse these categories into \"Democrats\", \"Independents\" and \"Republicans\". \n",
    "\n",
    "When we want to take an existing variable and combine responses to create something new, we can use the `map` method to apply a dictionary object that will re-map our original values onto new ones.The dictionary should take the form of `{[oldvalue]:[newvalue]}`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partyid_map = {\"1. Strong Democrat\": \"Democrat\",\n",
    "               \"2. Not very strong Democrat\": \"Democrat\",\n",
    "               \"3. Independent-Democrat\": \"Democrat\",\n",
    "               \"4. Independent\": \"Independent\", \n",
    "               \"5. Independent-Republican\": \"Republican\",\n",
    "               \"6. Not very strong Republican\": \"Republican\",\n",
    "               \"7. Strong Republican\": \"Republican\"\n",
    "}\n",
    "\n",
    "# using the assign function to create a new variable:\n",
    "df = df.assign(partyid_3cat= df.partyid.map(partyid_map))\n",
    "\n",
    "df.partyid_3cat.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since recoding is easy to mess up, I think its always a good idea to always:\n",
    "1. Preserve the old variable and give your new variable a different name\n",
    "2. Check your results by comparing the old variable to the new one.\n",
    "\n",
    "You can check your results by selecting the old values and the new values, dropping duplicates, and then checking to see how the unique labels match up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['partyid', 'partyid_3cat']].drop_duplicates().sort_values('partyid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping missing data\n",
    "\n",
    "In some cases, we will want to just drop rows that have missing data, we can use this with the Pandas `.dropna()` method. `axis=1` will drop columns with missing information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_missing_cols = df.dropna(axis = 1)\n",
    "df_no_missing_cols.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`axis=0` will drop all rows with missing information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_no_missing_rows = df.dropna(axis = 0)\n",
    "df_no_missing_rows.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to be careful here: surveys like the ANES will rarely ask every respondent to answer every single question, and some respondents will invariably skip or refuse to answer certain items. If we drop any rows with any missing data, we end up tossing out a huge amount of information: from 8280 responses to just 312. This is probably not a great idea. So more often we'll want only drop rows when some crucial bit of information is missing. For instance, we could do something like this to only drop rows where the `age_group` and `partyid_3cat` variables are missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_missing_pid_or_age = df.dropna(axis=0, subset=[\"partyid_3cat\", \"age_group\"])\n",
    "\n",
    "df_no_missing_pid_or_age.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we could only drop cases where there is non-missing data for at least two columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_min_2 = df.dropna(thresh=2, axis=0)\n",
    "\n",
    "df_min_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the previous examples, you should always be careful to avoid throwing out useful data if possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Data\n",
    "\n",
    "String data and needing to clean up string data are very common in social science research. Whether it's coding open response questions in surveys or parsing social media posts, lots of data in the social sciences are text data, and it's important to understand how to deal with them. This is particularly true when using web scraping techniques. The data that you get from web scraping will generally at least start out as string data, which you can then transform into numerical variables as needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions\n",
    "\n",
    "A **regular expression, or regex**, is a sequence of characters that are used to match patterns within text. These can be extremely useful for searching and matching complicated string patterns. Regexes use specific rules and formatting guidelines to specify various patterns and are implemented in Python via the `re` package. Let's take a look at a quick example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = 'this is some text'\n",
    "re.split(r\"\\s+\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regular expression `\\s+` refers to one of more whitespace characters. This uses `\\s` as the regex formatting for a whitespace character, as well as the `+` to indicate one or more. Note that `\\s` is used because `s` by itself would refer to the letter `s`. The backslash `\\` is an escape character that allows Python to interpret it as a pattern that it is trying to match.\n",
    "\n",
    "We could have done this without the `+` but that would try to separate on individual spaces instead. In the above example, there would be no difference, but if we were to add additional spaces, we'd see a big change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = 'this is some    text'\n",
    "re.split(r\"\\s\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also first compile a regular expression, then use it multiple times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "regex = re.compile(r\"\\s+\")\n",
    "regex.split(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "regex.split('some other  text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other regex functions, like `findall` can be used to extract bits of text that matches some pattern. This is an example of a pattern that could be used to identify and capture a price from a string of text. Notice I uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"33lbs of bananas cost $90.00\" \n",
    "\n",
    "re.findall(r\"(\\$[0-9]+[\\.0-9]{0,2})\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex methods\n",
    "\n",
    "|Method|Description|\n",
    "|-|-|\n",
    "|`findall`|Return all nonoverlapping matching patterns in a string as a list|\n",
    "|`match`|Match pattern at start of string and optionally segment pattern components into groups; if the pattern matches, return a match object, and otherwise None|\n",
    "|`search`|Scan string for match to pattern, returning a match object if so; unlike match, the match can be anywhere in the string as opposed to only at the beginning|\n",
    "|`split`|Break string into pieces at each occurrence of pattern|\n",
    "|`sub`|Replace all occurrences of pattern in string with replacement expression|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with regex\n",
    "\n",
    "Unless you work with regular expressions frequently, it will be very hard to get good enough at regex to write it out on the fly. Most of the time, you only need regex occasionally for one or two tasks, and it can be hard to remember what all of the patterns and syntax are. **You do not need to try to memorize all of the regex syntax!** There are plenty of tools available to help you out whenever you need to come back to it. Understanding some of the basics should help you get started, and you can use existing cheatsheets as well as online regex builder tools to do the rest. \n",
    "\n",
    "Regex 101 (https://regex101.com) is a website that helps you build your regexes. This website allows you to paste in the text you want to search, as well as type in a regex that you want to build. The text selected by the regex will be highlighted as you go, helping you build the exact regex that you need to get what you want. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Regex Cheatsheet\n",
    "\n",
    "Feel free to download this cheatsheet to more easily read it. This is also available on the course Canvas website.\n",
    "\n",
    "![Regex Cheatsheet](Regular_Expressions_Cheat_Sheet.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strings in DataFrames\n",
    "\n",
    "Many times, we'll have to work with strings that are in DataFrames. These are a bit trickier than individual strings, because we want to be efficient about how we do this. Luckily for us, the creators of the pandas DataFrames recognized that string manipulation would be fairly common and included tools to help make it easier.\n",
    "\n",
    "\n",
    "We can use the `extract` method to capture text that fits a regex pattern. The parentheses in the expression below are the \"capture group\" for our regex, this is the part of the pattern that we want to extract, so the code below will extract the portion of the text that matches the string \"liberal\", \"conservative\", or \"moderate\", so this is a way to recode the `libcon` variable to a 3-category measure without using the `map` function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first lower case, then match: \n",
    "libcon_lowercase = df.libcon.str.lower()\n",
    "\n",
    "df['libcon_3cat'] = libcon_lowercase.str.extract('(liberal|conservative|moderate)', re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare our new variables against the old ones like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['libcon', 'libcon_3cat']].drop_duplicates().sort_values('libcon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`str.contains` returns a `True` if the string contains the pattern given as its argument and `False` otherwise. One use for the `contains` function is to identify cases where a piece of text shows up so that you can filter or count the number of occurrences of that string. So if we wanted to know how many people mentioned climate change or global warming in their response to the \"most important political problem\" question, we could use a regular expression like `\"climate change|global warming\"` to count these cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =df.assign(climate_change = df['mipp_1'].str.contains('climate change|global warming', case=False, regex=True))\n",
    "\n",
    "# get the proportion who mentioned this: \n",
    "df.climate_change.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use this indicator to get the average proportion of people giving this response across different levels of some variable like party ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.pivot_table(index='partyid_3cat', values='climate_change', aggfunc='mean')\n",
    "# or : \n",
    "# df.groupby('partyid_3cat')['climate_change'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color ='red'>**Question 3: How would I show the relationship between `age_group` and the likelihood of mentioning Covid-19 as a response to the `mipp_1` question? Can you write your expression in a way that allows you to capture differences in spelling or punctuation?**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common use for the `str.contains` function is to subset a data set. Since it returns a boolean value, we can use it with `.loc` to get rows where a pattern is matched. For instance, here's how I would get a data frame containing only people who mentioned age, dementia, or cognitive ability in their reasons for disliking Joe Biden. (the `\\b` in this expression represents a word boundary, so this avoids capturing terms that just happen to begin or end with \"age\" like \"baggage\", or \"mortgage\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age = df.loc[df.biden_dislikes.str.contains(r'\\bold\\b|\\bage\\b|dementia|cogniti*', case=False, na=False)]\n",
    "\n",
    "# how many rows?\n",
    "df_age.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color ='red'>**Question 4: How many Democrats mentioned Donald Trump when asked to name a thing they liked about Joe Biden? How many Republicans mentioned Biden when asked to list something they liked about Trump?**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
