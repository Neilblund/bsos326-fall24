{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Python Workflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nyt-key.txt', 'r') as f:\n",
    "    nyt_key = f.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Workflow\n",
    "\n",
    "Recall that we used Python to pull from the NYT API. This involved specifying the URL, using an API key, and then wrangling that data into a DataFrame once that was all done. This is a process we might want to do multiple times, since we might want to get data for lots of different months or years. In other words, this is a prime candidate for a process that we might put into a function and then create a loop to create a database of New York Times articles.\n",
    "\n",
    "Organizing this process within a Jupyter notebook might be a bit messy though. We want to build a Python process of pulling from an API and wrangling the data, but we don't want to have to repeat that for every single notebook. You might have noticed this a bit in recent weeks when grabbing example datasets to use within notebooks. The process to actually get the data is a bit involved and is repeated within each notebook. Outside of a classroom setting, instead of putting all of that code into the Jupyter notebooks, we might instead use `.py` files to create scripts.\n",
    "\n",
    "There has been one made for you already. In this folder, there should be a file called `nyt_archive.py`. Open it and take a look. What is in this script? What do the functions do? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Functions\n",
    "\n",
    "Let's work with the functions that were created in this script. I can bring in the functions that I created there into this Jupyter notebook in a manner very similar to bringing in tools from Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nyt_archive import get_nyt_archive, get_nyt_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "abstracts_12_2019 = get_nyt_abstracts(12,2019,nyt_key)\n",
    "abstracts_12_2019.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `nyt_archive` is in the same folder as this Jupyter notebook. If it were in a folder, for example, we might use the relative path to import it, such as something like\n",
    "\n",
    "    from <folder>.nyt_archive import get_nyt_archive, get_nyt_abstracts\n",
    "    \n",
    "where `<folder>` represents the name of the folder in the same location as the Juypter notebook.\n",
    "\n",
    "This method means that we only need to define the functions for bringing in the NYT abstracts from the NYT Archive API once, and that we can then use these functions in other notebooks whenever we want. In other words, we don't need to keep copying and pasting the same code over and over again to use within other notebooks. This makes for a much more streamlined process. \n",
    "\n",
    "Now that we have these functions, we can use it to, for example, create a loop and make some CSV files with NYT Archive data. The `.to_csv()` method creates a CSV file with the file name and path provided in the first argument. The `index=False` makes it so that the index is not included as a column in the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for month in range(1,4):\n",
    "    get_nyt_abstracts(month,2019,nyt_key).to_csv(f'nyt_archive_{month}_2019.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separation of Workflow\n",
    "\n",
    "So, what should go in `.py` script files and what should go in Jupyter notebooks? In general, that depends on the purpose of what you are doing. Generally, when people use Python to code things like automated processes or bots or so on, they are only using `.py` files and not using Jupyter notebooks. However, Data Science is a bit different. In Data Science, not only are you doing visualizations and exploring data that you want to look at, but you also need to show the process, describe the analysis, and write down conclusions based on the code to have reproducible research. In other words, Data Science isn't just about the code. So, it makes sense to want to use a tool like Jupyter notebooks for doing \"real\" data science work. \n",
    "\n",
    "However, the best practice would probably not be to include everything within a Jupyter notebook. As mentioned above, if there are certain processes that you want to include in lots of notebooks or lots of analyses, then it makes sense to include them as functions within script files that you bring in. Or, if you are merely doing an automated data collection process (such as pulling from an API to build an archive of NYT articles), then there isn't really much \"analysis\" that you are doing about that. Once you build the archive and get to working with it, then you might include the code within a Jupyter notebook to make that process clear and reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Other Packages\n",
    "\n",
    "We've only used packages that were already available and installed in our BSOS JuypterHub environment so far. However, you might want to use packages that aren't already installed (or you might want to have JupyterLab on your own computers and need to install them yourself). To do this within Jupyter, you can use the `!` notation and use the pip installer to install any packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the Terminal window from Jupyter (or on your computer) to do the installation. Generally, if you find a package you want to use, it should provide instructions on how you can use the pip installer to install it. It might also include alternate methods for installation, such as conda. We won't go over the nuances of each in this class, but there are lots of resources that go over what might go into installing new packages into your environment or computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final note: Building a Database of Articles\n",
    "\n",
    "If we wanted to build a database of NYT articles, we might actually not store them as CSV files. Over enough months and years, this might be too unwieldy and hard to manage. In fact, at some point, it might be impossible to work with all of that data in Python. Python brings the data into **memory**, which means you are limited by the amount of RAM that you have. If you are working on your own computer, that would be the RAM that you have on that computer, and if you are on a JupyterHub server like this, you might be limited by how much RAM you have allocated to you. This means that once data gets into the order of multiple GBs, we would need to look for alternative solutions. So, instead, we might store the data in an **SQL Database**. \n",
    "\n",
    "We won't discuss SQL databases much in this course, but they are a good way of organizing and handling large amounts of data. Unlike Python, an SQL database does not need to bring the data into memory to manage it, and you will find that doing some basic data management tasks are much faster with large data in SQL than in Python. However, SQL lacks many of the powerful tools that Python has, such as visualizations, machine learning models, and more. Luckily for us, there are ways to work with Python and SQL databases together. \n",
    "\n",
    "You won't need to know about SQL databases for this course. However, this is something to keep in mind if you are ever in a situation that you want to use lots of data for a project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
